{"cells":[{"cell_type":"markdown","source":["The goal of this script is to compare DAX measures to corresponding SQL queries. \n"," \n","I have left some display() lines, feel free to use them to troubleshoot or walk through the code step-by-step.\n","\n","Here's a helpful article on the sempy functions: https://learn.microsoft.com/en-us/python/api/semantic-link-sempy/sempy.fabric?view=semantic-link-python#functions \n","\n","Full instructions available for how to do this from your laptop: https://dataonwheels.wordpress.com/?s=Power+BI%3A+Data+Quality+Checks+Using+Python+%26+SQL\n"," "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5e937aac-d0dd-468c-b16a-2af703426313"},{"cell_type":"code","source":["# Import Libraries\n","#%pip install semantic-link\n","import pandas as pd\n","import sempy.fabric as fabric\n","from sempy.fabric import FabricDataFrame\n","from sempy.dependencies import plot_dependency_metadata\n","from notebookutils import mssparkutils \n","from pyspark.sql.functions import concat_ws,col,lit,coalesce\n","from pyspark.sql import functions as F\n","from datetime import date, datetime\n","import pytz as tz "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false},"editable":false,"run_control":{"frozen":true}},"id":"59124566-e787-41c1-80a1-59cb8493b18b"},{"cell_type":"markdown","source":["###### Calling another notebook in this workspace that contains some common functions that we can share among other notebooks\n","###### This notebook also contains all the library imports needed"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b2455c91-01b7-4d2f-af10-ede8c3216c5d"},{"cell_type":"code","source":["%run nb_functions"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0b5e6c93-cd32-47ab-a111-3e07c464e348"},{"cell_type":"code","source":["#Set up some variables\n","\n","v_dataset = \"Biking Sales in Hobbiton Fabric\" #can be swapped for GUIDs\n","v_workspace = \"Fabric of Middle-Earth\" #can be swapped for GUIDs\n","v_lakehouse = \"lh_hobbiton_data\"\n","v_tz = tz.timezone('UTC')\n","v_run_datetime = dt.now(v_tz) #grabs the current date and time in UTC"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05e7ace5-1fa2-4b66-80a7-70fe6da0da7c"},{"cell_type":"code","source":["# How can we run PBI DAX measures?\n","\n","df = fabric.evaluate_measure(\n","    dataset=\"Biking Sales in Hobbiton Fabric\", #can be swapped for GUIs\n","    measure=\"Cost\", #can have multiple measures here\n","    workspace=\"Fabric of Middle-Earth\")\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"4af32272-cc60-41d8-ab06-3b389e8695a3"},{"cell_type":"code","source":["# How can we run SQL queries against tables in our Lakehouse?\n","df_sql = spark.sql(\"SELECT * FROM lh_hobbiton_data.aw_sales LIMIT 10\")\n","display(df_sql)\n","df_sql_alias = spark.sql(\"SELECT SUM(Order_Quantity) as order_quantity FROM lh_hobbiton_data.aw_sales\")\n","display(df_sql_alias)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9d4f2785-c35e-42b8-9781-13fd412bf115"},{"cell_type":"code","source":["# How can we easily loop through a list of measures and SQL queries to check? \n","\n","# Start by creating a dataframe that houses the values we want to check\n","\n","df_checker = spark.createDataFrame(\n","    [  #create data here, be consistent with types\n","        #metric = lower(replace(pbi_measure,\" \",\"_\")) = sql_query column name\n","        (\"cost\",\"Cost\",\"SELECT SUM(Total_Product_Cost) AS cost FROM lh_hobbiton_data.aw_sales\")\n","        ,(\"quantity_ordered\",\"Quantity Ordered\",\"SELECT SUM(Order_Quantity) as quantity_ordered FROM lh_hobbiton_data.aw_sales\")\n","        ,(\"profit_margin\",\"Profit Margin\",\"SELECT SUM(Sales_Amount) - SUM(Total_Product_Cost) as profit_margin FROM lh_hobbiton_data.aw_sales\")\n","    ], \n","    [\"metric\",\"pbi_measure\",\"sql_query\"] # initial column names here (we will add some after this later in the query)\n",")\n","\n","display(df_checker)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d4288798-65ff-4b8d-b7b6-845de5343d20"},{"cell_type":"code","source":["# Now loop over the pbi_measures and sql_queries to compare the two values\n","\n","# To do this, we are going to create a list of our PBI Measures using a collect function. This will allow us to loop over them.  \n","for row in df_checker.rdd.collect():\n","    #create variables to grab current record that we are looping over\n","    current_measure = row.pbi_measure\n","    current_metric = row.metric\n","    #execute the current pbi measure\n","    df_pbi = fabric.evaluate_measure(\n","        dataset=\"Biking Sales in Hobbiton Fabric\", #you can use names or guids\n","        measure=current_measure, \n","        workspace=\"Fabric of Middle-Earth\")\n","    df_pbi=spark.createDataFrame(df_pbi)\n","    pbi_name = str('pbi_' + current_metric)\n","    #rename the pbi measure fields and add a column to join to the og df\n","    df_pbi = df_pbi.withColumnRenamed(current_measure, str(pbi_name)).drop(current_measure).withColumn(\"new_measure\",lit(current_measure))\n","    display(df_pbi)\n","    df_checker = df_checker.join(df_pbi,df_checker.pbi_measure == df_pbi.new_measure, \"leftouter\").drop(df_pbi.new_measure)\n","    \n","    #now loop over the sql queries\n","    df_sql = spark.sql(row.sql_query)\n","    sql_name = str('sql_' + current_metric)\n","    #use current_metric instead of current_measure since the sql column name should match the metric field\n","    df_sql = df_sql.withColumnRenamed(current_metric,str(sql_name)).drop(current_measure).withColumn(\"new_query\",lit(current_metric))\n","    display(df_sql)\n","    df_checker = df_checker.join(df_sql,df_checker.metric == df_sql.new_query, \"leftouter\").drop(df_sql.new_query)\n","    #display(df_checker)\n","display(df_checker)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"c4d68445-3030-4874-a842-1ec65b5c47dc"},{"cell_type":"code","source":["# now we will convert to pandas dataframe in order to coalesce some values. \n","# Splitting our column sets into two new dfs, first we will coalesce the power bi values\n","\n","df_pd_checker = df_checker.toPandas()\n","# Exclude columns with prefix 'sql_'\n","    # ~ is essentially a NOT function\n","df_pbi_clean_up = df_pd_checker.loc[:, ~df_pd_checker.columns.str.startswith('sql_')]\n","\n","#coalesce columns without naming them\n","    #iloc[:,2:] selects all rows (:) and allows us to skip the first two columns for the coalesce \n","    #bfill stands for backward fill. It will fill all NULL values backward along our chosen axis\n","    #axis = 1 means the operation is performed among the columns, so for each row it fills in NaN values with the next non-NaN value in the row.\n","    #iloc[:,0] occurs after the backward fill. It selects the first column of the new df. This will contain our coalesce\n","df_pbi_clean_up['pbi_result'] = df_pbi_clean_up.iloc[:, 2:].bfill(axis=1).iloc[:, 0]\n","\n","#time to pick only our final columns\n","df_pbi_clean_up = df_pbi_clean_up[['metric','pbi_measure','pbi_result']]\n","#print(df_pbi_clean_up)\n","\n","#back to spark df\n","df_pbi_clean_up = spark.createDataFrame(df_pbi_clean_up)\n","display(df_pbi_clean_up)\n","\n","#time to clean up our sql columns\n","df_sql_clean_up = df_pd_checker.loc[:, ~df_pd_checker.columns.str.startswith('pbi_')]\n","df_sql_clean_up['sql_result'] = df_sql_clean_up.iloc[:, 2:].bfill(axis=1).iloc[:, 0]\n","df_sql_clean_up.rename(columns={'metric': 'sql_metric'}, inplace=True) #need to avoid creating dup columns in the final dataframe\n","df_sql_clean_up = df_sql_clean_up[['sql_metric','sql_query','sql_result']]\n","df_sql_clean_up = spark.createDataFrame(df_sql_clean_up)\n","display(df_sql_clean_up)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"536ea045-44f2-40aa-b3c7-e4c05bcae5a5"},{"cell_type":"code","source":["#let's bring our two dataframes back together and calc the difference!\n","\n","df_check = df_pbi_clean_up.join(df_sql_clean_up,df_sql_clean_up.sql_metric == df_pbi_clean_up.metric)\n","df_check = df_check[['metric','pbi_measure','pbi_result','sql_query','sql_result']]\n","#creating a column to calc the difference, but rounding a bit to avoid any issues due to rounding in the tools\n","df_check = df_check.withColumn('difference',round(df_check['pbi_result'],4) - round(df_check['sql_result'],4))\n","display(df_check)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a5f99cb7-391d-4e86-9066-8d4ccfc34193"},{"cell_type":"code","source":["#time to write these results to our lakehouse\n","#let's add some metadata fields to our dataframe\n","df_check = df_check.withColumn('run_datetime',lit(v_run_datetime)).withColumn('semantic_model',lit(v_dataset)).withColumn('workspace',lit(v_workspace))\n","display(df_check)\n","\n","# Write the DataFrame to a Delta table\n","delta_table_path = \"Tables/data_quality_log\"\n","df_check.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").save(delta_table_path)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d598d149-4eaf-4e3b-a20e-7b932be1db84"},{"cell_type":"code","source":["# Syncing the lakehouse to the SQL Endpoint to ensure that any downstream processes will grab the proper records\n","\n","\n","sync_result = udf_SyncSqlEndpoint(v_workspace,v_lakehouse)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"517201ef-c56f-42f3-8762-f21cce0f016c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"b847b276-a251-440f-b56b-0c4c53b50251","default_lakehouse_name":"lh_hobbiton_data","default_lakehouse_workspace_id":"253a31af-7442-450f-b93a-680a4831c265"}}},"nbformat":4,"nbformat_minor":5}