{"cells":[{"cell_type":"code","source":["from pyspark.sql import Window\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from delta import *\n","import sempy.fabric as fabric\n","from datetime import date, datetime as dt\n","import ast\n","import notebookutils\n","from functools import reduce\n","from random import randint\n","from time import sleep\n","from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n","import json\n","import time\n","import re\n","import pandas as pd\n","from sempy.fabric import FabricDataFrame\n","from sempy.dependencies import plot_dependency_metadata\n","import pytz as tz \n","\n","\n","\n","# Configuration\n","spark.conf.set('spark.sql.parquet.int96RebaseModeInWrite','LEGACY')\n","spark.conf.set('spark.sql.parquet.int96RebaseModeInRead','LEGACY')\n","spark.conf.set('spark.sql.parquet.datetimeRebaseModeInRead','LEGACY')\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":119,"statement_ids":[119],"state":"finished","livy_statement_state":"available","session_id":"1e9d8c98-b606-47a6-aab6-28b1e5d4c192","normalized_state":"finished","queued_time":"2025-02-06T14:53:44.9969785Z","session_start_time":null,"execution_start_time":"2025-02-06T14:53:45.1676822Z","execution_finish_time":"2025-02-06T14:53:45.4214945Z","parent_msg_id":"667bdacd-b890-4bdc-ba9c-df08013ac84b"},"text/plain":"StatementMeta(, 1e9d8c98-b606-47a6-aab6-28b1e5d4c192, 119, Finished, Available, Finished)"},"metadata":{}}],"execution_count":117,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5bf331b-767b-49f4-a625-21781e9be3ae"},{"cell_type":"code","source":["# file path function\n","def udf_GetFilePath (workspace, lakehouse, table):\n","\n","    workspaceID = fabric.list_workspaces( f\"name eq '{workspace}'\")['Id'][0]\n","         \n","    lakehousePath = notebookutils.lakehouse.get( name = lakehouse, workspaceId = workspaceID )['properties']['abfsPath']\n","\n","\n","    return f'{lakehousePath}/Tables/{table}'"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"340ea0e4-1d32-41df-ba04-770fed94c849"},{"cell_type":"code","source":["# metadata functions\n","\n","\n","\n","def udf_AddHashKeyColumn (df, naturalKeyColumnList) :\n","    \n","    # compile list of columns to include in hash, and exclude the natural key(s) of the table\n","    hashColumns = [c for c in df.columns if c not in naturalKeyColumnList]\n","    \n","    # hash column logic: replace nulls with the string 'NA', concatenate columns with a pipe separator, and create hash\n","    hash_column = sha2(concat_ws(\"|\", *[when(col(c).isNotNull(), col(c).cast(StringType())).otherwise(lit('NA')) for c in hashColumns]), 256)\n","    \n","    # return the dataframe with the hash column\n","    return df.select(\n","                \"*\"\n","                ,hash_column.alias(\"ETLHashKey\"))\n","    \n","def udf_AddSCD2Columns (df) :\n","\n","    # add Start/End datetime columns, IsCurrent flag\n","    return df.select(\n","                \"*\"\n","                ,lit(\"1900-01-01\").cast(\"timestamp\").alias(\"StartDateTime\")\n","                ,lit(\"9999-12-31\").cast(\"timestamp\").alias(\"EndDateTime\")\n","                ,lit(True).alias(\"IsCurrent\")\n","                )\n","\n","def udf_AddModifiedDateColumn (df) :\n","\n","    # add  modified datetime column\n","    return df.select(\n","                \"*\"\n","                ,current_timestamp().alias(\"ETLModifiedDateTime\")\n","                )\n","\n","def udf_AddPrimaryKey (df, targetPath, primaryKeyColumnName=\"ID\") :\n","\n","    # default the max existing ID to 0 (in the event of an initial load)\n","    maxID = 0\n","\n","    # find the largest ID if the table exists  \n","    if notebookutils.fs.exists(targetPath):\n","        maxID = spark.read.format(\"delta\").load( targetPath ).agg({primaryKeyColumnName:\"max\"}).collect()[0][0]\n","\n","    return df.select((maxID + row_number().over(Window.orderBy(lit(None)))).alias(primaryKeyColumnName)\n","            ,\"*\"\n","            )\n","                    "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"session_starting","livy_statement_state":null,"session_id":null,"normalized_state":"session_starting","queued_time":"2025-02-07T14:58:06.093592Z","session_start_time":"2025-02-07T14:58:06.0949617Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"0d6d8c8d-ff27-4f7d-8446-2741014e4845"},"text/plain":"StatementMeta(, , -1, SessionStarting, , SessionStarting)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b7b4ab9e-4f2c-49ee-96b2-0c29ba71fb7d"},{"cell_type":"code","source":["def udf_LoadTableInitial (df, targetPath) :\n","\n","    # write data to gold lakehouse\n","    (\n","        df\n","            .write\n","            .format( 'delta' )\n","            .mode( 'overwrite' )\n","            .option('delta.columnMapping.mode', 'name')\n","            .option( 'overwriteSchema', 'True' )\n","            .save( targetPath )\n","    )"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":123,"statement_ids":[123],"state":"finished","livy_statement_state":"available","session_id":"1e9d8c98-b606-47a6-aab6-28b1e5d4c192","normalized_state":"finished","queued_time":"2025-02-06T14:53:45.3330807Z","session_start_time":null,"execution_start_time":"2025-02-06T14:53:50.2823943Z","execution_finish_time":"2025-02-06T14:53:50.5388171Z","parent_msg_id":"591b9c52-939a-4148-b6f5-c50482954947"},"text/plain":"StatementMeta(, 1e9d8c98-b606-47a6-aab6-28b1e5d4c192, 123, Finished, Available, Finished)"},"metadata":{}}],"execution_count":121,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4313ac5b-6209-4544-8153-e0ba05418dae"},{"cell_type":"code","source":["def udf_UpsertDimension(df, dimensionType, targetPath, naturalKeyColumnList, primaryKeyColumnName=\"ID\", flagSoftDeletes=False):\n","    \n","    # collect stats for logging\n","    startTime = dt.now()\n","    staging_rows_added = df.count()\n","\n","    # default some strings that we'll need in logic further down to operate for a type 1 dimension. We'll overwrite them for Type 2 SCDs next if applicable.\n","    dfTargetWhereClause = \"1=1\"\n","    mergeCondition = \"1=1 AND \"\n","    updateCondition = \"1=1\"\n","\n","    # create filter strings to limit to current rows if we are loading a type 2 dimension\n","    if dimensionType==2:\n","        dfTargetWhereClause = \"IsCurrent = 1\"\n","        mergeCondition =  f\"source.`{primaryKeyColumnName}` IS NULL AND target.IsCurrent = 1 AND \"\n","        # for the merge statement below; \"updates\" in a type 2 will not update the entire record but rather expire a record\n","        updateSetClause = {\"EndDateTime\": \"source.EndDateTime\",\n","        \"IsCurrent\": \"source.IsCurrent\",\n","        \"ETLModifiedDateTime\": \"source.ETLModifiedDateTime\"}\n","\n","    # for soft deletes, deleted flag will be treated like a normal field and included in the hash\n","    if flagSoftDeletes:\n","\n","        # if records exist in the source DF, they are not deleted\n","        df = df.withColumn(\"IsDeleted\",lit(False))\n","\n","        # if this is not the initial load, pull in target data and join to source data to add deleted records which are no longer in the source\n","        if notebookutils.fs.exists(targetPath):\n","            dfTarget = (\n","            spark.read.format(\"delta\").load( targetPath )\n","            .select(*naturalKeyColumnList)\n","            .where(dfTargetWhereClause)\n","            )\n","\n","            deletes = (\n","                dfTarget\n","                .alias(\"target\")\n","                .join(df.alias(\"source\"), naturalKeyColumnList, \"left_anti\")\n","                .withColumn(\"IsDeleted\",lit(True))\n","            )\n","\n","            df = df.unionByName(deletes,allowMissingColumns=True)\n","\n","    # first, take the source dataset and add a hash key\n","    df = udf_AddHashKeyColumn( df, naturalKeyColumnList )\n","\n","    # if we are loading a type 2 dimension, add start/end/is current columns\n","    if dimensionType==2:\n","        df = udf_AddSCD2Columns( df )\n","\n","    # add ETL Modified Date\n","    df = udf_AddModifiedDateColumn( df )\n","\n","\n","    # check if the target path exists; if not, load the data for the first time\n","    if notebookutils.fs.exists(targetPath):\n","\n","\n","        targetDelta = DeltaTable.forPath( spark, targetPath )\n","\n","        dfTarget = (\n","        spark.read.format(\"delta\").load( targetPath )\n","        .select(*naturalKeyColumnList,col(\"ETLHashKey\").alias(\"targetHashKey\"),col(primaryKeyColumnName).alias(\"targetID\"))\n","        .where(dfTargetWhereClause)\n","        )\n","\n","        # limit source dataset to rows which differ from target or are new entirely\n","        updates = (\n","            df\n","            .alias(\"source\")\n","            .join(dfTarget.alias(\"target\"), naturalKeyColumnList, \"inner\")\n","            .where(\"source.ETLHashKey != target.targetHashKey\")\n","        )\n","\n","        # if this is a type 2 dimension, updates result in expiring the current row\n","        if dimensionType==2:\n","            expires = (\n","                updates\n","                .withColumn(\"IsCurrent\",lit(False))\n","                .withColumn(\"EndDateTime\",current_timestamp())\n","            )\n","\n","        inserts = (\n","            df\n","            .alias(\"source\")\n","            .join(dfTarget.alias(\"target\"), naturalKeyColumnList, \"left\")\n","            .where(\"target.targetHashKey IS NULL\")\n","        )\n","        \n","        #if this is a type 2 dimension, inserts are really the new record for updates and brand new dimension records altogether\n","        if dimensionType==2:\n","            inserts = (\n","                inserts\n","                .union(updates.withColumn(\"StartDateTime\",current_timestamp()))\n","            )\n","            updates = expires\n","\n","        inserts = udf_AddPrimaryKey(inserts, targetPath, primaryKeyColumnName)\n","\n","        mergeCondition += \" AND \".join([f\"target.`{c}` = source.`{c}`\" for c in naturalKeyColumnList])\n","\n","        insertsCount = inserts.count()\n","        updatesCount = updates.count()\n","\n","        stageChanges = (\n","            inserts\n","            .union(\n","                updates\n","                .select(\n","                    lit(None).cast(IntegerType()).alias(primaryKeyColumnName)\n","                    ,\"*\"\n","                )\n","            )\n","            .drop(\"targetHashKey\",\"targetID\")\n","        )\n","\n","\n","        if dimensionType == 1:\n","            # we created this clause for type 2 dimensions above, but for type 1 we'll want to update every column except for the primary key\n","            updateSetClause = {f\"`{c}`\": f\"source.`{c}`\" for c in stageChanges.columns if c != primaryKeyColumnName}\n","\n","        # perform updates and inserts using merge statement\n","        (targetDelta.alias(\"target\").merge(\n","            source = stageChanges.alias(\"source\"),\n","            condition = mergeCondition\n","        )\n","        .whenMatchedUpdate(set=updateSetClause)\n","        .whenNotMatchedInsertAll().execute()\n","        )\n","        print('Upsert complete')\n","\n","    else:\n","\n","        df = udf_AddPrimaryKey(df, targetPath, primaryKeyColumnName)\n","        udf_LoadTableInitial(df, targetPath)\n","        insertsCount = df.count()\n","        updatesCount = 0\n","        print('Initial load complete')    \n","\n","\n","    # Return output to caller for logging\n","    stopTime = dt.now()\n","    details = f'{updatesCount} records updated, {insertsCount} records inserted from {staging_rows_added} staging rows'\n","    returnVal = {\n","        'startTime': str( startTime ),\n","        'stopTime': str( stopTime ),\n","        'details': str( details)\n","    }\n","\n","    return returnVal"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8b38f252-81b6-47f1-8e84-2f8489cf6d9c"},{"cell_type":"code","source":["# Function to sync SQL endpoint with the Lakehouse\n","def udf_SyncSqlEndpoint(workspace, lakehouse):\n","    logs = []\n","    try:\n","        # Fetch SQL endpoint properties\n","\n","        workspaceID = fabric.list_workspaces( f\"name eq '{workspace}'\")['Id'][0]\n","        lakehouseID = notebookutils.lakehouse.get( name = lakehouse, workspaceId = workspaceID )['id']\n","\n","        client = fabric.FabricRestClient()\n","        lakehouse_info = client.get(f\"/v1/workspaces/{workspaceID}/lakehouses/{lakehouseID}\").json()\n","        sql_endpoint_id = lakehouse_info['properties']['sqlEndpointProperties']['id']\n","        \n","        # Set URI for the API call\n","        uri = f\"/v1.0/myorg/lhdatamarts/{sql_endpoint_id}\"\n","        payload = {\"commands\": [{\"$type\": \"MetadataRefreshExternalCommand\"}]}\n","        \n","        # Call REST API to initiate the sync\n","        response = client.post(uri, json=payload)\n","        if response.status_code != 200:\n","            logs.append(f\"Error initiating sync: {response.status_code} - {response.text}\")\n","            return\n","        \n","        data = json.loads(response.text)\n","\n","        batch_id = data[\"batchId\"]\n","        progress_state = data[\"progressState\"]\n","\n","        # URL for checking the sync status\n","        status_uri = f\"/v1.0/myorg/lhdatamarts/{sql_endpoint_id}/batches/{batch_id}\"\n","        \n","        # Polling until the sync is complete\n","        while progress_state == 'inProgress':\n","            time.sleep(1)  # Polling interval\n","            status_response = client.get(status_uri)\n","            status_data = status_response.json()\n","            progress_state = status_data[\"progressState\"]\n","        \n","        # Check if the sync completed successfully\n","        if progress_state == 'success':\n","            table_details = [\n","                {\n","                    'tableName': table['tableName'],\n","                    'lastSuccessfulUpdate': table.get('lastSuccessfulUpdate', 'N/A'),\n","                    'tableSyncState': table['tableSyncState'],\n","                    'sqlSyncState': table['sqlSyncState'],\n","                    'warningMessages': table['warningMessages']\n","                }\n","                for table in status_data['operationInformation'][0]['progressDetail']['tablesSyncStatus']\n","            ]\n","            \n","            # Print extracted table details\n","            for detail in table_details:\n","                print(f\"Table: {detail['tableName']}   Last Update: {detail['lastSuccessfulUpdate']}  \"\n","                      f\"Table Sync State: {detail['tableSyncState']}  SQL Sync State: {detail['sqlSyncState']}   \"\n","                      f\"Table Warnings: {detail['warningMessages']}\")\n","                logs.append(f\"Table: {detail['tableName']}   Last Update: {detail['lastSuccessfulUpdate']}  \"\n","                      f\"Table Sync State: {detail['tableSyncState']}  SQL Sync State: {detail['sqlSyncState']}   \"\n","                      f\"Table Warnings: {detail['warningMessages']}\")\n","            #uncomment if you need to see all the details\n","            #display(status_data)\n","            \n","        # Handle failure\n","        elif progress_state == 'failure':\n","            logs.append(f\"Sync failed: {status_data}\")\n","    \n","    except FabricHTTPException as fe:\n","        logs.append(f\"Fabric HTTP Exception: {fe}\")\n","    except WorkspaceNotFoundException as we:\n","        logs.append(f\"Workspace not found: {we}\")\n","    except Exception as e:\n","        logs.append(f\"An unexpected error occurred: {e}\")\n","    return(logs)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2527ee77-1751-43d4-bf77-14cb1cd663fe"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"warehouse":{}}},"nbformat":4,"nbformat_minor":5}