{"cells":[{"cell_type":"code","source":["# Libraries # \n","import json, requests, pandas as pd \n","from datetime import datetime as dt, timedelta\n","from pyspark.sql import SparkSession, Row, Window, functions as funcast\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from requests.exceptions import HTTPError\n","from dateutil.parser import parse as dtparser\n","from pyspark.sql.types import StringType\n","\n","import sempy.fabric as fabric \n","from notebookutils import notebook  \n","from notebookutils.mssparkutils.handlers.notebookHandler import RunMultipleFailedException\n","import notebookutils.mssparkutils as mssparkutils\n","\n","from delta import *\n","import ast\n","import notebookutils\n","from functools import reduce\n","from random import randint\n","from time import sleep\n","from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n","import time\n","import re\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","# Not importing sempy_labs because we can't run %pip install from a run multiple notebook scenario. \n","# recreated the calls below in udfs\n","\n","#try:\n","#    import sempy_labs as labs\n","#except:\n","#    %pip install semantic-link-labs\n","#    import sempy_labs as labs\n","#%pip install semantic-link-labs\n","\n","#import sempy_labs as labs\n","#\n","#from sempy_labs import migration, directlake, admin, graph, lakehouse as lake, report as rep\n","#from sempy_labs.admin import _activities\n","#from sempy_labs.tom import connect_semantic_model\n","#from sempy_labs.report import ReportWrapper\n","#import sempy_labs._icons as icons\n","#from sempy_labs._helper_functions import (\n","#    resolve_workspace_name_and_id,\n","#    resolve_capacity_id,\n","#    _base_api,\n","#    _create_dataframe\n","#)\n","#from sempy_labs.admin import (_activities, list_activity_events, _tenant)\n","#from sempy_labs._refresh_semantic_model import (get_semantic_model_refresh_history)\n","\n","\n","\n","# Configuration\n","spark.conf.set('spark.sql.parquet.int96RebaseModeInWrite','LEGACY')\n","spark.conf.set('spark.sql.parquet.int96RebaseModeInRead','LEGACY')\n","spark.conf.set('spark.sql.parquet.datetimeRebaseModeInRead','LEGACY')\n","\n","\n","fab_client = fabric.FabricRestClient()\n","pbi_client = fabric.PowerBIRestClient()\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false}},"id":"e808dc32-42df-47e9-8b42-e7a70852c492"},{"cell_type":"markdown","source":["#### Sempy Library UDFs (no pip install needed)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"493c26bf-1afb-4276-a81b-e90f50d0b30b"},{"cell_type":"code","source":["# sempy library UDFs\n","\n","\n","def generate_hourly_windows(date_str):\n","    windows = []\n","    start = dt.strptime(date_str, \"%Y-%m-%d\")\n","    for i in range(24):\n","        hour_start = start + timedelta(hours=i)\n","        hour_end = hour_start + timedelta(hours=1) - timedelta(milliseconds=1)\n","\n","        start_str = hour_start.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n","        end_str = hour_end.strftime(\"%Y-%m-%dT%H:%M:%S.999Z\")\n","\n","        windows.append((start_str, end_str))\n","    return windows\n","\n","\n","def resolve_workspace_name_and_id(workspace_name, df_workspaces):\n","    \"\"\"\n","    Returns a tuple of (workspaceName, workspaceId) from a known workspace list.\n","    \"\"\"\n","    row = df_workspaces.filter(df_workspaces[\"name\"] == workspace_name).first()\n","    if row:\n","        return row[\"name\"], row[\"id\"]\n","    else:\n","        raise ValueError(f\"Workspace '{workspace_name}' not found.\")\n","\n","def pagination(response, headers):\n","    responses = []\n","    \n","    try:\n","        response_json = response.json()\n","    except Exception as e:\n","        print(f\"⚠️ Failed to parse JSON for URL: {response.url}\")\n","        print(f\"🔍 Status code: {response.status_code}\")\n","        print(f\"📄 Raw response text: {response.text[:300]}\")  # limit output\n","        raise e  # re-raise or handle gracefully\n","\n","    responses.append(response_json)\n","\n","    # Follow @odata.nextLink for pagination\n","    while '@odata.nextLink' in response_json:\n","        next_url = response_json['@odata.nextLink']\n","        response = requests.get(next_url, headers=headers)\n","        try:\n","            response_json = response.json()\n","            responses.append(response_json)\n","        except Exception as e:\n","            print(f\"⚠️ Pagination failed at {next_url}\")\n","            print(f\"🔍 Status code: {response.status_code}\")\n","            print(f\"📄 Raw response: {response.text[:300]}\")\n","            break  # or raise, depending on preference\n","\n","    return responses\n","'''\n","def pagination(response,headers):\n","\n","    responses = []\n","    response_json = response.json()\n","    responses.append(response_json)\n","\n","    # Check for pagination\n","    continuation_token = response_json.get(\"continuationToken\")\n","    continuation_uri = response_json.get(\"continuationUri\")\n","\n","    # Loop to handle pagination\n","    while continuation_token is not None:\n","        response = requests.get(continuation_uri, headers=headers)\n","        response_json = response.json()\n","        responses.append(response_json)\n","\n","        # Update the continuation token and URI for the next iteration\n","        continuation_token = response_json.get(\"continuationToken\")\n","        continuation_uri = response_json.get(\"continuationUri\")\n","\n","    return responses\n","'''\n","\n","def _base_api(request, method=\"get\", payload=None, headers=None, uses_pagination=False):\n","    base_url = \"https://api.powerbi.com\"\n","    url = base_url + request\n","\n","    token = mssparkutils.credentials.getToken(\"https://analysis.windows.net/powerbi/api\")\n","\n","    if headers is None:\n","        headers = {\n","            \"Content-Type\": \"application/json\",\n","            \"Authorization\": f\"Bearer {token}\"\n","        }\n","\n","    if method.lower() == \"get\":\n","        response = requests.get(url, headers=headers)\n","        if uses_pagination:\n","            responses = pagination(response,headers)\n","            return responses\n","        else:\n","            return response\n","    elif method.lower() == \"post\":\n","        response = requests.post(url, json=payload, headers=headers)\n","    else:\n","        raise ValueError(f\"Unsupported method: {method}\")\n","\n","    response.raise_for_status()\n","    return response\n","\n","\n","'''\n","def list_activity_events(start_date, end_date):\n","    request = f\"/v1.0/myorg/admin/activityevents?startDateTime={start_date}&endDateTime={end_date}\"\n","    response = _base_api(request, method=\"get\")\n","    return response.json().get(\"value\", [])\n","'''\n","\n","def _create_dataframe(columns: dict) -> pd.DataFrame:\n","    return pd.DataFrame(columns=list(columns.keys()))\n","\n","def _update_dataframe_datatypes(dataframe: pd.DataFrame, column_map: dict):\n","    \"\"\"\n","    Updates the datatypes of columns in a pandas dataframe based on a column map.\n","\n","    Example:\n","    {\n","        \"Order\": \"int\",\n","        \"Public\": \"bool\",\n","    }\n","    \"\"\"\n","\n","    for column, data_type in column_map.items():\n","        if column in dataframe.columns:\n","            if data_type == \"int\":\n","                dataframe[column] = dataframe[column].astype(int)\n","            elif data_type == \"bool\":\n","                dataframe[column] = dataframe[column].astype(bool)\n","            elif data_type == \"float\":\n","                dataframe[column] = dataframe[column].astype(float)\n","            elif data_type == \"datetime\":\n","                dataframe[column] = pd.to_datetime(dataframe[column])\n","            # This is for a special case in admin.list_reports where datetime itself does not work. Coerce fixes the issue.\n","            elif data_type == \"datetime_coerce\":\n","                dataframe[column] = pd.to_datetime(dataframe[column], errors=\"coerce\")\n","            # This is for list_synonyms since the weight column is float and can have NaN values.\n","            elif data_type == \"float_fillna\":\n","                dataframe[column] = dataframe[column].fillna(0).astype(float)\n","            # This is to avoid NaN values in integer columns (for delta analyzer)\n","            elif data_type == \"int_fillna\":\n","                dataframe[column] = dataframe[column].fillna(0).astype(int)\n","            elif data_type in [\"str\", \"string\"]:\n","                dataframe[column] = dataframe[column].astype(str)\n","            else:\n","                raise NotImplementedError\n","\n","def list_activity_events(\n","    start_time: str,\n","    end_time: str,\n","    activity_filter: Optional[str] = None,\n","    user_id_filter: Optional[str] = None,\n","    return_dataframe: bool = True,\n",") -> pd.DataFrame:\n","    start_dt = dtparser(start_time)\n","    end_dt = dtparser(end_time)\n","\n","    if not start_dt.date() == end_dt.date():\n","        raise ValueError(f\"{icons.red_dot} Start and End Times must be within the same UTC day.\")\n","\n","    columns = {\n","        \"Id\": \"string\", \"Record Type\": \"string\", \"Creation Time\": \"datetime\",\n","        \"Operation\": \"string\", \"Organization Id\": \"string\", \"User Type\": \"string\",\n","        \"User Key\": \"string\", \"Workload\": \"string\", \"Result Status\": \"string\",\n","        \"User Id\": \"string\", \"Client IP\": \"string\", \"User Agent\": \"string\",\n","        \"Activity\": \"string\", \"Workspace Name\": \"string\", \"Workspace Id\": \"string\",\n","        \"Object Id\": \"string\", \"Request Id\": \"string\", \"Object Type\": \"string\",\n","        \"Object Display Name\": \"string\", \"Experience\": \"string\",\n","        \"Refresh Enforcement Policy\": \"string\", \"Is Success\": \"bool\",\n","        \"Activity Id\": \"string\", \"Item Name\": \"string\", \"Dataset Name\": \"string\",\n","        \"Report Name\": \"string\", \"Capacity Id\": \"string\", \"Capacity Name\": \"string\",\n","        \"App Name\": \"string\", \"Dataset Id\": \"string\", \"Report Id\": \"string\",\n","        \"Artifact Id\": \"string\", \"Artifact Name\": \"string\", \"Report Type\": \"string\",\n","        \"App Report Id\": \"string\", \"Distribution Method\": \"string\",\n","        \"Consumption Method\": \"string\", \"Artifact Kind\": \"string\",\n","    }\n","\n","    df = _create_dataframe(columns=columns)\n","    url = f\"/v1.0/myorg/admin/activityevents?startDateTime='{start_time}'&endDateTime='{end_time}'\"\n","\n","    filters = []\n","    if activity_filter:\n","        filters.append(f\"Activity eq '{activity_filter}'\")\n","    if user_id_filter:\n","        filters.append(f\"UserId eq '{user_id_filter}'\")\n","    if filters:\n","        url += f\"&$filter={' and '.join(filters)}\"\n","\n","    try:\n","        responses = _base_api(request=url, uses_pagination=True)\n","    except Exception as e:\n","        print(f\"⚠️ Error in API call for {start_time}–{end_time}: {e}\")\n","        return df  # empty DataFrame fallback\n","\n","    rows = []\n","    for response in responses:\n","        for event in response.get(\"activityEventEntities\", []):\n","            rows.append({k: event.get(k.replace(\" \", \"\")) for k in columns})\n","\n","    if rows:\n","        df = pd.DataFrame(rows)\n","        _update_dataframe_datatypes(df, columns)\n","\n","    return df\n","\n","\n","\n","\n","\n","def get_semantic_model_refresh_history(workspace_id, dataset_id):\n","    request = f\"/v1.0/myorg/groups/{workspace_id}/datasets/{dataset_id}/refreshes\"\n","    response = _base_api(request, method=\"get\")\n","    return response.json().get(\"value\", [])\n","\n","\n","class icons:\n","    green_dot = \"🟢\"\n","    red_dot = \"🔴\"\n","    yellow_dot = \"🟡\"\n","    check_mark = \"✅\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d742d6c-28ec-4d20-983f-8620975b2927"},{"cell_type":"markdown","source":["#### Functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3193c5ea-7853-4d48-8045-c32f8b87e4ba"},{"cell_type":"code","source":["def grant_admin_access(workspaceId, current_user):\n","    payload = {\n","            \"emailAddress\": current_user,\n","            \"groupUserAccessRight\": \"Admin\",\n","            \"principalType\": \"User\",\n","            \"identifier\": current_user,\n","        }\n","    #grant access\n","    _base_api(\n","        request=f\"/v1.0/myorg/admin/groups/{workspaceId}/users\",\n","        method=\"post\",\n","        payload=payload,\n","    )\n","    return workspaceId "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ac745811-67f0-4bb0-a116-ee713f728ac9"},{"cell_type":"code","source":["def union_batches(df_list, batch_size=50):\n","    \"\"\"\n","    Efficiently unions a large list of Spark DataFrames in batches.\n","    This avoids deep execution plans caused by chaining thousands of .unionByName() calls.\n","    \n","    Parameters:\n","        df_list (list): List of Spark DataFrames to union.\n","        batch_size (int): Number of DataFrames to union at a time.\n","\n","    Returns:\n","        Spark DataFrame: A single DataFrame resulting from the union of all inputs.\n","    \"\"\"\n","    if not df_list:\n","        return None\n","\n","    while len(df_list) > 1:\n","        batched = []\n","        for i in range(0, len(df_list), batch_size):\n","            batch = df_list[i:i + batch_size]\n","            batch_union = reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), batch)\n","            batched.append(batch_union)\n","        df_list = batched  # Now reduce over fewer, bigger DataFrames\n","    return df_list[0]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"1095e3f8-0ab6-4152-80b2-f37dd4dcbb47","normalized_state":"finished","queued_time":"2025-04-18T16:46:46.2627818Z","session_start_time":null,"execution_start_time":"2025-04-18T16:47:12.7642519Z","execution_finish_time":"2025-04-18T16:47:13.1599372Z","parent_msg_id":"eef0fa40-42c8-4c3f-9274-b10db2eb453b"},"text/plain":"StatementMeta(, 1095e3f8-0ab6-4152-80b2-f37dd4dcbb47, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e2104c64-74aa-4aab-9d67-63fd296a3e15"},{"cell_type":"code","source":["def get_workspace_items(workspace=None):\n","    df_items = None\n","    if workspace is None:\n","        workspaceId = fabric.get_workspace_id() #gets this workspace by default\n","    else:\n","        try:\n","            workspaceId = fabric.resolve_workspace_id(workspace)\n","            response = fab_client.get(f\"/v1/workspaces/{workspaceId}/items\")\n","\n","            # Check the status code of the response for this endpoint\n","            # Use 200 if operation is completed, 201 if item is created\n","            if response.status_code != 200:\n","                raise FabricHTTPException(response)\n","\n","            df_items = pd.json_normalize(response.json()['value'])\n","        except WorkspaceNotFoundException as e:\n","            print(\"Caught a WorkspaceNotFoundException:\", e)\n","        except FabricHTTPException as e:\n","            print(\"Caught a FabricHTTPException. Check the API endpoint, authentication.\")\n","    return df_items\n","   "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"1095e3f8-0ab6-4152-80b2-f37dd4dcbb47","normalized_state":"finished","queued_time":"2025-04-18T16:46:46.3728182Z","session_start_time":null,"execution_start_time":"2025-04-18T16:47:13.1621845Z","execution_finish_time":"2025-04-18T16:47:13.4670919Z","parent_msg_id":"31d2fe9d-e8f0-4c28-9519-3679158126b4"},"text/plain":"StatementMeta(, 1095e3f8-0ab6-4152-80b2-f37dd4dcbb47, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3c0138bf-7777-4632-aeac-b6c3dd924554"},{"cell_type":"code","source":["def replace_nulls_in_array(arr, default=\"UNKNOWN\"):\n","    if arr is None:\n","        return arr\n","    return [x if x is not None else default for x in arr]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"1095e3f8-0ab6-4152-80b2-f37dd4dcbb47","normalized_state":"finished","queued_time":"2025-04-18T16:46:46.4377711Z","session_start_time":null,"execution_start_time":"2025-04-18T16:47:13.4694467Z","execution_finish_time":"2025-04-18T16:47:13.7778821Z","parent_msg_id":"db3859d3-192d-4d19-83e0-33315fa68111"},"text/plain":"StatementMeta(, 1095e3f8-0ab6-4152-80b2-f37dd4dcbb47, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eeb3da2c-444a-4663-bf8d-4a5d1dc8fc20"},{"cell_type":"code","source":["# metadata functions\n","\n","def udf_AddHashKeyColumn (df, naturalKeyColumnList) :\n","    \n","    # compile list of columns to include in hash, and exclude the natural key(s) of the table\n","    hashColumns = [c for c in df.columns if c not in naturalKeyColumnList]\n","    \n","    # Ensure all hash columns are converted to string, replace nulls with 'NA'\n","    exprs = [\n","        when(col(c).isNotNull(), col(c).cast(StringType()))\n","        .otherwise(lit(\"NA\"))\n","        for c in hashColumns\n","    ]\n","\n","    # Generate the hash column using SHA2 over a pipe-delimited string\n","    hash_column = sha2(concat_ws(\"|\", *exprs), 256)\n","\n","    # Return original DataFrame with new hash column\n","    return df.select(\"*\", hash_column.alias(\"ETLHashKey\"))\n","    \n","    \n","def udf_AddSCD2Columns (df) :\n","\n","    # add Start/End datetime columns, IsCurrent flag\n","    return df.select(\n","                \"*\"\n","                ,lit(\"1900-01-01\").cast(\"timestamp\").alias(\"StartDateTime\")\n","                ,lit(\"9999-12-31\").cast(\"timestamp\").alias(\"EndDateTime\")\n","                ,lit(True).alias(\"IsCurrent\")\n","                )\n","\n","def udf_AddModifiedDateColumn (df) :\n","\n","    # add  modified datetime column\n","    return df.select(\n","                \"*\"\n","                ,current_timestamp().alias(\"ETLModifiedDateTime\")\n","                )\n","\n","def udf_AddPrimaryKey (df, targetPath, primaryKeyColumnName=\"ID\") :\n","\n","    # default the max existing ID to 0 (in the event of an initial load)\n","    maxID = 0\n","\n","    # find the largest ID if the table exists  \n","    if notebookutils.fs.exists(targetPath):\n","        maxID = spark.read.format(\"delta\").load( targetPath ).agg({primaryKeyColumnName:\"max\"}).collect()[0][0]\n","\n","    return df.select((maxID + row_number().over(Window.orderBy(lit(None)))).alias(primaryKeyColumnName)\n","            ,\"*\"\n","            )\n","                    "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"1095e3f8-0ab6-4152-80b2-f37dd4dcbb47","normalized_state":"finished","queued_time":"2025-04-18T16:46:46.4898097Z","session_start_time":null,"execution_start_time":"2025-04-18T16:47:13.7800021Z","execution_finish_time":"2025-04-18T16:47:14.1020537Z","parent_msg_id":"98b1df17-3812-4261-84d8-5a1b4730f35b"},"text/plain":"StatementMeta(, 1095e3f8-0ab6-4152-80b2-f37dd4dcbb47, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5b781a77-27b2-46ba-b794-2846f2b81647"},{"cell_type":"code","source":["def udf_LoadTableInitial (df, targetPath) :\n","\n","    # write data to gold lakehouse\n","    (\n","        df\n","            .write\n","            .format( 'delta' )\n","            .mode( 'overwrite' )\n","            .option('delta.columnMapping.mode', 'name')\n","            .option( 'overwriteSchema', 'True' )\n","            .save( targetPath )\n","    )"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"1095e3f8-0ab6-4152-80b2-f37dd4dcbb47","normalized_state":"finished","queued_time":"2025-04-18T16:46:46.5490114Z","session_start_time":null,"execution_start_time":"2025-04-18T16:47:14.1043542Z","execution_finish_time":"2025-04-18T16:47:14.4175707Z","parent_msg_id":"0b91b055-a538-469c-ad88-7ad32fc923e8"},"text/plain":"StatementMeta(, 1095e3f8-0ab6-4152-80b2-f37dd4dcbb47, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2b033328-386d-4b0d-9552-b32e0a67e4eb"},{"cell_type":"code","source":["# file path function\n","def udf_GetFilePath (workspace, lakehouse, table):\n","    workspaceID = fabric.list_workspaces( f\"name eq '{workspace}'\")['Id'][0]\n","    lakehousePath = notebookutils.lakehouse.get( name = lakehouse, workspaceId = workspaceID )['properties']['abfsPath']\n","    return f'{lakehousePath}/Tables/{table}'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"1095e3f8-0ab6-4152-80b2-f37dd4dcbb47","normalized_state":"finished","queued_time":"2025-04-18T16:46:46.6108164Z","session_start_time":null,"execution_start_time":"2025-04-18T16:47:14.4196992Z","execution_finish_time":"2025-04-18T16:47:14.735519Z","parent_msg_id":"5d62e91b-8b78-42fe-aff2-e2f9e48e4635"},"text/plain":"StatementMeta(, 1095e3f8-0ab6-4152-80b2-f37dd4dcbb47, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5f4b3c5e-600d-47dd-9938-a85efba34afe"},{"cell_type":"code","source":["def udf_UpsertDimension(df, dimensionType, targetPath, naturalKeyColumnList, primaryKeyColumnName=\"ID\", flagSoftDeletes=False):\n","    from datetime import datetime as dt\n","    startTime = dt.now()\n","\n","    staging_rows_added = None\n","    if not df.rdd.isEmpty():\n","        staging_rows_added = df.count()\n","\n","    dfTargetWhereClause = \"1=1\"\n","    mergeCondition = \"1=1 AND \"\n","    updateSetClause = {}  # init outside if blocks\n","\n","    if dimensionType == 2:\n","        dfTargetWhereClause = \"IsCurrent = 1\"\n","        mergeCondition = f\"source.`{primaryKeyColumnName}` IS NULL AND target.IsCurrent = 1 AND \"\n","        updateSetClause = {\n","            \"EndDateTime\": \"source.EndDateTime\",\n","            \"IsCurrent\": \"source.IsCurrent\",\n","            \"ETLModifiedDateTime\": \"source.ETLModifiedDateTime\"\n","        }\n","\n","    # Soft deletes optimization — avoid unnecessary joins\n","    if flagSoftDeletes and notebookutils.fs.exists(targetPath):\n","        df = df.withColumn(\"IsDeleted\", lit(False))\n","\n","        dfTarget = (\n","            spark.read.format(\"delta\").load(targetPath)\n","            .select(*naturalKeyColumnList)\n","            .where(dfTargetWhereClause)\n","        )\n","\n","        deletes = (\n","            dfTarget.alias(\"target\")\n","            .join(df.alias(\"source\"), naturalKeyColumnList, \"left_anti\")\n","            .withColumn(\"IsDeleted\", lit(True))\n","        )\n","\n","        df = df.unionByName(deletes, allowMissingColumns=True)\n","\n","    # Add columns\n","    df = udf_AddHashKeyColumn(df, naturalKeyColumnList)\n","    if dimensionType == 2:\n","        df = udf_AddSCD2Columns(df)\n","    df = udf_AddModifiedDateColumn(df)\n","\n","    # Upsert logic\n","    if notebookutils.fs.exists(targetPath):\n","        targetDelta = DeltaTable.forPath(spark, targetPath)\n","\n","        # Selecting only relevant columns\n","        dfTarget = (\n","            spark.read.format(\"delta\").load(targetPath)\n","            .select(\n","                *naturalKeyColumnList,\n","                col(\"ETLHashKey\").alias(\"targetHashKey\"),\n","                col(primaryKeyColumnName).alias(\"targetID\")\n","            )\n","            .where(dfTargetWhereClause)\n","        ).cache()\n","\n","        # Use cache to prevent recomputation\n","        df = df.cache()\n","\n","        updates = (\n","            df.alias(\"source\")\n","            .join(dfTarget.alias(\"target\"), naturalKeyColumnList, \"inner\")\n","            .where(\"source.ETLHashKey != target.targetHashKey\")\n","        )\n","\n","        if dimensionType == 2:\n","            expires = (\n","                updates\n","                .withColumn(\"IsCurrent\", lit(False))\n","                .withColumn(\"EndDateTime\", current_timestamp())\n","            )\n","\n","        inserts = (\n","            df.alias(\"source\")\n","            .join(dfTarget.alias(\"target\"), naturalKeyColumnList, \"left\")\n","            .where(\"target.targetHashKey IS NULL\")\n","        )\n","\n","        if dimensionType == 2:\n","            inserts = inserts.union(updates.withColumn(\"StartDateTime\", current_timestamp()))\n","            updates = expires\n","\n","        inserts = udf_AddPrimaryKey(inserts, targetPath, primaryKeyColumnName)\n","\n","        mergeCondition += \" AND \".join([f\"target.`{c}` = source.`{c}`\" for c in naturalKeyColumnList])\n","\n","        # Avoid unnecessary count()s\n","        insertsCount = None\n","        updatesCount = None\n","        if not inserts.rdd.isEmpty():\n","            insertsCount = inserts.count()\n","        if not updates.rdd.isEmpty():\n","            updatesCount = updates.count()\n","\n","        stageChanges = (\n","            inserts\n","            .union(\n","                updates.select(\n","                    lit(None).cast(IntegerType()).alias(primaryKeyColumnName), \"*\"\n","                )\n","            )\n","            .drop(\"targetHashKey\", \"targetID\")\n","        )\n","\n","        if dimensionType == 1:\n","            updateSetClause = {\n","                f\"`{c}`\": f\"source.`{c}`\" for c in stageChanges.columns if c != primaryKeyColumnName\n","            }\n","\n","        # Optimization 5: Use one merge call, or optionally split\n","        (\n","            targetDelta.alias(\"target\").merge(\n","                source=stageChanges.alias(\"source\"),\n","                condition=mergeCondition\n","            )\n","            .whenMatchedUpdate(set=updateSetClause)\n","            .whenNotMatchedInsertAll()\n","            .execute()\n","        )\n","\n","        print(\"✅ Upsert complete\")\n","\n","    else:\n","        df = udf_AddPrimaryKey(df, targetPath, primaryKeyColumnName)\n","        udf_LoadTableInitial(df, targetPath)\n","        insertsCount = df.count()\n","        updatesCount = 0\n","        print(\"📦 Initial load complete\")\n","\n","    stopTime = dt.now()\n","    details = f'{updatesCount or 0} records updated, {insertsCount or 0} records inserted from {staging_rows_added or \"?\"} staging rows to {targetPath}'\n","\n","    return {\n","        \"startTime\": str(startTime),\n","        \"stopTime\": str(stopTime),\n","        \"details\": details\n","    }\n","\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"1095e3f8-0ab6-4152-80b2-f37dd4dcbb47","normalized_state":"finished","queued_time":"2025-04-18T16:46:46.6695455Z","session_start_time":null,"execution_start_time":"2025-04-18T16:47:14.7380791Z","execution_finish_time":"2025-04-18T16:47:15.0927903Z","parent_msg_id":"7a8bc017-1b60-4d8e-a105-3a75476ebb0a"},"text/plain":"StatementMeta(, 1095e3f8-0ab6-4152-80b2-f37dd4dcbb47, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a82a275d-3b25-4bfb-a670-3bdbea1fb2b4"},{"cell_type":"code","source":["# Function to sync SQL endpoint with the Lakehouse or Warehouse\n","def udf_SyncSqlEndpoint(workspace, lakehouse):\n","    logs = []\n","    try:\n","        # Fetch SQL endpoint properties\n","\n","        workspaceID = fabric.list_workspaces( f\"name eq '{workspace}'\")['Id'][0]\n","        lakehouseID = notebookutils.lakehouse.get( name = lakehouse, workspaceId = workspaceID )['id']\n","\n","        client = fabric.FabricRestClient()\n","        lakehouse_info = client.get(f\"/v1/workspaces/{workspaceID}/lakehouses/{lakehouseID}\").json()\n","        sql_endpoint_id = lakehouse_info['properties']['sqlEndpointProperties']['id']\n","        \n","        # Set URI for the API call\n","        uri = f\"/v1.0/myorg/lhdatamarts/{sql_endpoint_id}\"\n","        payload = {\"commands\": [{\"$type\": \"MetadataRefreshExternalCommand\"}]}\n","        \n","        # Call REST API to initiate the sync\n","        response = client.post(uri, json=payload)\n","        if response.status_code != 200:\n","            logs.append(f\"Error initiating sync: {response.status_code} - {response.text}\")\n","            return\n","        \n","        data = json.loads(response.text)\n","\n","        batch_id = data[\"batchId\"]\n","        progress_state = data[\"progressState\"]\n","\n","        # URL for checking the sync status\n","        status_uri = f\"/v1.0/myorg/lhdatamarts/{sql_endpoint_id}/batches/{batch_id}\"\n","        \n","        # Polling until the sync is complete\n","        while progress_state == 'inProgress':\n","            time.sleep(1)  # Polling interval\n","            status_response = client.get(status_uri)\n","            status_data = status_response.json()\n","            progress_state = status_data[\"progressState\"]\n","        \n","        # Check if the sync completed successfully\n","        if progress_state == 'success':\n","            table_details = [\n","                {\n","                    'tableName': table['tableName'],\n","                    'lastSuccessfulUpdate': table.get('lastSuccessfulUpdate', 'N/A'),\n","                    'tableSyncState': table['tableSyncState'],\n","                    'sqlSyncState': table['sqlSyncState'],\n","                    'warningMessages': table['warningMessages']\n","                }\n","                for table in status_data['operationInformation'][0]['progressDetail']['tablesSyncStatus']\n","            ]\n","            \n","            # Print extracted table details\n","            for detail in table_details:\n","                print(f\"Table: {detail['tableName']}   Last Update: {detail['lastSuccessfulUpdate']}  \"\n","                      f\"Table Sync State: {detail['tableSyncState']}  SQL Sync State: {detail['sqlSyncState']}   \"\n","                      f\"Table Warnings: {detail['warningMessages']}\")\n","                logs.append(f\"Table: {detail['tableName']}   Last Update: {detail['lastSuccessfulUpdate']}  \"\n","                      f\"Table Sync State: {detail['tableSyncState']}  SQL Sync State: {detail['sqlSyncState']}   \"\n","                      f\"Table Warnings: {detail['warningMessages']}\")\n","            #uncomment if you need to see all the details\n","            #display(status_data)\n","            \n","        # Handle failure\n","        elif progress_state == 'failure':\n","            logs.append(f\"Sync failed: {status_data}\")\n","    \n","    except FabricHTTPException as fe:\n","        logs.append(f\"Fabric HTTP Exception: {fe}\")\n","    except WorkspaceNotFoundException as we:\n","        logs.append(f\"Workspace not found: {we}\")\n","    except Exception as e:\n","        logs.append(f\"An unexpected error occurred: {e}\")\n","    return(logs)\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"1095e3f8-0ab6-4152-80b2-f37dd4dcbb47","normalized_state":"finished","queued_time":"2025-04-18T16:46:46.7569789Z","session_start_time":null,"execution_start_time":"2025-04-18T16:47:15.0953043Z","execution_finish_time":"2025-04-18T16:47:15.378534Z","parent_msg_id":"bc112254-1939-47c1-b661-801df44ec326"},"text/plain":"StatementMeta(, 1095e3f8-0ab6-4152-80b2-f37dd4dcbb47, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"68205f77-c071-4d32-afdc-ba6745c317e9"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}