{"cells":[{"cell_type":"markdown","source":["### Purpose\n","##### Goal: Load workspaces, datasets, reports, and activity events for the Entegris tenant\n","##### Notes:\n","- Only works if the owner/runner of the notebook is a tenant admin\n","- Will automatically assign the runner of the notebook to all workspaces in order to get the refresh information (no admin api available to run this)\n","##### Helpful Resources:\n","- https://learn.microsoft.com/en-us/python/api/semantic-link-sempy/sempy.fabric?view=semantic-link-python\n","- https://community.fabric.microsoft.com/t5/Data-Engineering-Community-Blog/Spark-Connector-for-Fabric-Warehouse-Unified-Analytics/ba-p/4611309\n","- https://stackoverflow.com/questions/71001110/power-bi-rest-api-requests-not-authorizing-as-expected\n","- To use KeyVault, refer to this article: https://darren.gosbell.com/2023/06/calling-a-power-bi-rest-api-from-a-fabric-notebook/"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70bf340d-e0b4-4984-825e-ffb2259f1a1c"},{"cell_type":"code","source":["# Imports\n","import notebookutils as nb\n","from ast import literal_eval\n","import sempy.fabric as fabric\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","from datetime import datetime as dt\n","from builtins import filter as stdFilter\n","from notebookutils.mssparkutils.handlers.notebookHandler import RunMultipleFailedException\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import time\n","import json, requests, pandas as pd \n","from requests.exceptions import HTTPError\n","fab_client = fabric.FabricRestClient()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","normalized_state":"finished","queued_time":"2025-10-07T21:49:07.8187218Z","session_start_time":null,"execution_start_time":"2025-10-07T21:49:17.6802626Z","execution_finish_time":"2025-10-07T21:49:22.9437712Z","parent_msg_id":"5b2ad814-fdb1-4f80-a332-0d94c316ad69"},"text/plain":"StatementMeta(, cbff0b2d-78d1-47a2-883f-ee2ebb194048, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8671e303-f38f-427f-9b88-a85099cdc353"},{"cell_type":"code","source":["# Variables\n","workspace = 'Admin%20Center' #have to escape the & symbol and spaces\n","lakehouse = 'lh_monitoring'\n","\n","ws_table = 'dimWorkspaces'\n","workspace_users_table = 'dimWorkspaceUsers'\n","tenant_settings = 'dimTenantSettings'\n","capacities_table = 'dimCapacities'\n","dashboard_table = 'dimDashboards'\n","dataflow_table = 'dimDataflows'\n","report_table = 'dimReports'\n","\n","dataset_table = 'dimSemanticModels'\n","refreshHist_table = 'factRefreshHistory'\n","refreshSched_table = 'factRefreshSchedule'\n","\n","act_table = 'factActivities'\n","#defaults the activity data to grab yesterday's data. The API only goes back 28 days maximum.\n","date_offset = 1\n","current_user = mssparkutils.env.getUserName()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","normalized_state":"finished","queued_time":"2025-10-07T21:49:27.7908038Z","session_start_time":null,"execution_start_time":"2025-10-07T21:49:27.7920307Z","execution_finish_time":"2025-10-07T21:49:28.7576829Z","parent_msg_id":"9146c575-d0b5-4fcf-be8c-43add34698ab"},"text/plain":"StatementMeta(, cbff0b2d-78d1-47a2-883f-ee2ebb194048, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3829575c-1670-469b-8a21-9a603f654b26"},{"cell_type":"markdown","source":["## Tasks to Run in Parallel"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"61e59eb3-1537-41c6-a0d4-5112589c56e2"},{"cell_type":"code","source":["#path is the name of the notebook to trigger\n","#args is where we pass parameters from this notebook into the sub notebooks\n","\n","nb_to_run = [\n","    {\"path\": \"nb_run_workspaces\", \"args\":{\"workspace\":workspace,\"lakehouse\":lakehouse,\"ws_table\":ws_table,\"workspace_users_table\":workspace_users_table}} ,\n","    {\"path\": \"nb_run_tenant_settings\", \"args\":{\"workspace\":workspace,\"lakehouse\":lakehouse,\"tenant_settings\":tenant_settings}} ,\n","    {\"path\": \"nb_run_capacities\", \"args\":{\"workspace\":workspace,\"lakehouse\":lakehouse,\"capacities_table\":capacities_table}} ,\n","    {\"path\": \"nb_run_dashboards\", \"args\":{\"workspace\":workspace,\"lakehouse\":lakehouse,\"dashboard_table\":dashboard_table}} ,\n","    {\"path\": \"nb_run_dataflows\", \"args\":{\"workspace\":workspace,\"lakehouse\":lakehouse,\"dataflow_table\":dataflow_table}} ,\n","    {\"path\": \"nb_run_reports\", \"args\":{\"workspace\":workspace,\"lakehouse\":lakehouse,\"report_table\":report_table}} ,\n","    {\"path\": \"nb_run_datasets\", \"args\":{\"workspace\":workspace,\"lakehouse\":lakehouse,\"dataset_table\":dataset_table,\"refreshHist_table\":refreshHist_table,\"refreshSched_table\":refreshSched_table}} \n","    ,{\"path\": \"nb_run_activities\", \"args\":{\"workspace\":workspace,\"lakehouse\":lakehouse,\"act_table\":act_table,\"date_offset\":date_offset}} \n","    ]\n","\n","# Use this to backload activity data if needed \n","#nb_to_run = [{\"path\": \"nb_run_activities\", \"args\":{\"workspace\":workspace,\"lakehouse\":lakehouse,\"act_table\":act_table,\"date_offset\" = date_offset}}]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","normalized_state":"finished","queued_time":"2025-10-07T21:49:30.75861Z","session_start_time":null,"execution_start_time":"2025-10-07T21:49:30.7598388Z","execution_finish_time":"2025-10-07T21:49:31.1302153Z","parent_msg_id":"e1400085-776c-43aa-b43e-48becee73c7b"},"text/plain":"StatementMeta(, cbff0b2d-78d1-47a2-883f-ee2ebb194048, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d86100a5-e012-4026-aaeb-86684e63a1fb"},{"cell_type":"markdown","source":["## Adding admin access for the current user. \n","- NOTE: if there are more than 200 workspaces you need access to, we can only grant access to 200 workspaces an hour so this will take a while since I've built in a wait function"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"65add511-5461-4303-9901-5c4f4fe683f0"},{"cell_type":"code","source":["#get a list of all workspaces and load to a table in the lakehouse\n","response = fab_client.get(f\"/v1/admin/workspaces\")\n","df_workspaces = pd.json_normalize(response.json()['workspaces'])\n","#df_workspaces\n","df_workspaces = spark.createDataFrame(df_workspaces)\n","\n","#creates a list of workspaces we want access to for dataset refresh, history, and workspace users\n","df_np_workspaces = df_workspaces \\\n","    .filter(df_workspaces[\"type\"] == \"Workspace\")  \\\n","    .filter(df_workspaces[\"state\"] ==\"Active\") #\\\n","    #.filter(df_workspaces[\"id\"] != '5d683bbb-9a09-492a-814e-2e444f53a4dd')\n","\n","#df_np_workspaces.head(20)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"session_error","livy_statement_state":null,"session_id":null,"normalized_state":"session_error","queued_time":"2025-10-08T01:51:18.8769612Z","session_start_time":"2025-10-08T01:51:18.8779321Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"1e7e6b8e-7ae3-4308-9188-50fd6cb6d491"},"text/plain":"StatementMeta(, , -1, SessionError, , SessionError)"},"metadata":{}},{"output_type":"error","ename":"InvalidHttpRequestToLivy","evalue":"[TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430.","traceback":["InvalidHttpRequestToLivy: [TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430."]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6a45a614-5f6d-425b-927d-91b99a598e97"},{"cell_type":"code","source":["def _base_api(request, method=\"get\", payload=None, headers=None):\n","    base_url = \"https://api.powerbi.com\"\n","    url = base_url + request\n","\n","    token = mssparkutils.credentials.getToken(\"https://analysis.windows.net/powerbi/api\")\n","\n","    if headers is None:\n","        headers = {\n","            \"Content-Type\": \"application/json\",\n","            \"Authorization\": f\"Bearer {token}\"\n","        }\n","\n","    if method.lower() == \"get\":\n","        response = requests.get(url, headers=headers)\n","    elif method.lower() == \"post\":\n","        response = requests.post(url, json=payload, headers=headers)\n","    else:\n","        raise ValueError(f\"Unsupported method: {method}\")\n","\n","    response.raise_for_status()\n","    return response"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","normalized_state":"finished","queued_time":"2025-10-07T21:50:49.6412625Z","session_start_time":null,"execution_start_time":"2025-10-07T21:50:49.6424269Z","execution_finish_time":"2025-10-07T21:50:49.9263249Z","parent_msg_id":"c0a045c5-5be9-4768-b172-4686a32f157d"},"text/plain":"StatementMeta(, cbff0b2d-78d1-47a2-883f-ee2ebb194048, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6eda3387-0de2-4916-9e05-a5f37d68fff7"},{"cell_type":"code","source":["rows_workspaces = df_np_workspaces.collect()\n","admin_api_limit = 200\n","admin_calls_made = 0\n","granted_workspaces = []\n","skipped_workspaces = []\n","payload = {\n","            \"emailAddress\": current_user,\n","            \"groupUserAccessRight\": \"Admin\",\n","            \"principalType\": \"User\",\n","            \"identifier\": current_user,\n","        }\n","\n","def user_already_has_access(workspaceId):\n","    try:\n","        response = _base_api(\n","            request=f\"/v1.0/myorg/groups/{workspaceId}/users\",\n","            method=\"get\"\n","        )\n","        users = response.json().get(\"value\", [])\n","        return any(user.get(\"identifier\", \"\").lower() == current_user.lower() for user in users)\n","    except HTTPError as e:\n","        if e.response.status_code == 429:\n","            retry_after = int(e.response.headers.get(\"Retry-After\", 3600))\n","            print(f\"â›”ï¸ Hit 429 rate limit for CHECKING access. Sleeping for {retry_after} seconds...\")\n","            time.sleep(retry_after)\n","            # Retry once after sleeping\n","            response = _base_api(\n","            request=f\"/v1.0/myorg/groups/{workspaceId}/users\",\n","            method=\"get\"\n","            )\n","\n","def process_workspace(row):\n","    global admin_calls_made\n","    workspaceId = row[\"id\"]\n","    workspaceName = row[\"name\"]\n","\n","    try:\n","        if user_already_has_access(workspaceId):\n","            print(f\"âœ… Already has access to {workspaceName}\")\n","            skipped_workspaces.append(workspaceName)\n","            return\n","\n","        if admin_calls_made >= admin_api_limit:\n","            print(\"â³ Reached admin API limit. Sleeping for 1 hour...\")\n","            time.sleep(3600)\n","            admin_calls_made = 0\n","\n","        try:\n","            _base_api(\n","                request=f\"/v1.0/myorg/admin/groups/{workspaceId}/users\",\n","                method=\"post\",\n","                payload=payload\n","            )\n","            admin_calls_made += 1\n","            granted_workspaces.append(workspaceName)\n","            print(f\"âœ… Access granted to {workspaceName}\")\n","\n","        except HTTPError as e:\n","            if e.response.status_code == 429:\n","                retry_after = int(e.response.headers.get(\"Retry-After\", 3600))\n","                print(f\"â›”ï¸ Hit 429 rate limit for GRANTING access. Sleeping for {retry_after} seconds...\")\n","                time.sleep(retry_after)\n","                # Retry once after sleeping\n","                _base_api(\n","                    request=f\"/v1.0/myorg/admin/groups/{workspaceId}/users\",\n","                    method=\"post\",\n","                    payload=payload\n","                )\n","                admin_calls_made += 1\n","                granted_workspaces.append(workspaceName)\n","                print(f\"âœ… Access granted to {workspaceName} after retry\")\n","            else:\n","                raise e  # Re-raise for anything else\n","\n","    except Exception as e:\n","        print(f\"âŒ Failed for {workspaceName}: {e}\")\n","\n","# Run with threads (adjust max_workers if needed)\n","with ThreadPoolExecutor(max_workers=10) as executor:\n","    futures = {executor.submit(process_workspace, row): row for row in rows_workspaces}\n","    for future in as_completed(futures):\n","        pass  # Output is handled in the function\n","\n","print(\"âœ… Done granting access!\")\n","print(f\"ðŸ”“ Granted access to {len(granted_workspaces)} workspaces.\")\n","print(f\"â­ï¸ Skipped {len(skipped_workspaces)} workspaces (already had access).\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","normalized_state":"finished","queued_time":"2025-10-07T21:50:53.2182935Z","session_start_time":null,"execution_start_time":"2025-10-07T21:50:53.2196746Z","execution_finish_time":"2025-10-07T21:50:54.8382332Z","parent_msg_id":"55aad0bf-9e85-40fb-8c65-df68c42e366b"},"text/plain":"StatementMeta(, cbff0b2d-78d1-47a2-883f-ee2ebb194048, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Already has access to Operational Performance\nâœ… Already has access to Member Resources - Claims\nâœ… Already has access to Internal Resources\nâœ… Already has access to Member Resources - Incidents\nâœ… Already has access to Microsoft 365 Usage Analytics\nâœ… Already has access to AMBI Test Workspace\nâœ… Already has access to User Test Workspace\nâœ… Already has access to Unemployment Pool\nâœ… Already has access to AESD\nâœ… Already has access to Claims Demographics\nâœ… Already has access to Admin Center\nâœ… Access granted to EDgage - Prod\nâœ… Access granted to EDgage - Dev\nâœ… Access granted to LEP\nâœ… Done granting access!\nðŸ”“ Granted access to 3 workspaces.\nâ­ï¸ Skipped 11 workspaces (already had access).\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7a74c6e1-5a73-47ca-a020-a1864ffdab8e"},{"cell_type":"markdown","source":["## Run Tasks\n","##### Using run multiple allows us to run the following notebooks in parallel and within the same spark session (makes it run much faster and uses less capacity)\n","\n","If you need to run just one, comment out the other notebooks. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"adf031e4-47dc-4436-99fd-5cfd53532cca"},{"cell_type":"code","source":["# Build DAG String from metadata\n","df_nb_to_run = spark.createDataFrame(nb_to_run)\n","\n","activities = []\n","[\n","    activities.append(\n","        {\n","            'name': m.path,\n","            'path': m.path,\n","            'timeoutPerCellInSeconds': 60000,\n","        }\n","    )\n","    for m in df_nb_to_run.collect()\n","]\n","\n","\n","DAG = {\n","    'activities': activities,\n","    'timeoutInSeconds': 60000, # Number of seconds allowed for all activities to complete before shutting down\n","    'concurrency': 20 # Max number of notebooks to run concurrently\n","}\n","print(DAG)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","normalized_state":"finished","queued_time":"2025-10-07T21:51:05.7563783Z","session_start_time":null,"execution_start_time":"2025-10-07T21:51:05.757468Z","execution_finish_time":"2025-10-07T21:51:08.3861027Z","parent_msg_id":"18e64518-7eb4-4cbd-a10f-c8b71596fa41"},"text/plain":"StatementMeta(, cbff0b2d-78d1-47a2-883f-ee2ebb194048, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'activities': [{'name': 'nb_run_workspaces', 'path': 'nb_run_workspaces', 'timeoutPerCellInSeconds': 60000}, {'name': 'nb_run_tenant_settings', 'path': 'nb_run_tenant_settings', 'timeoutPerCellInSeconds': 60000}, {'name': 'nb_run_capacities', 'path': 'nb_run_capacities', 'timeoutPerCellInSeconds': 60000}, {'name': 'nb_run_dashboards', 'path': 'nb_run_dashboards', 'timeoutPerCellInSeconds': 60000}, {'name': 'nb_run_dataflows', 'path': 'nb_run_dataflows', 'timeoutPerCellInSeconds': 60000}, {'name': 'nb_run_reports', 'path': 'nb_run_reports', 'timeoutPerCellInSeconds': 60000}, {'name': 'nb_run_datasets', 'path': 'nb_run_datasets', 'timeoutPerCellInSeconds': 60000}, {'name': 'nb_run_activities', 'path': 'nb_run_activities', 'timeoutPerCellInSeconds': 60000}], 'timeoutInSeconds': 60000, 'concurrency': 20}\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5fcc6fc8-5c92-42ee-935f-7e9ae40643b3"},{"cell_type":"code","source":["try:\n","    output = notebookutils.notebook.runMultiple( DAG, {'displayDAGViaGraphviz': False} )\n","except RunMultipleFailedException as e:\n","    output = e.result\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","normalized_state":"finished","queued_time":"2025-10-07T21:58:23.6755586Z","session_start_time":null,"execution_start_time":"2025-10-07T21:58:23.6767404Z","execution_finish_time":"2025-10-07T21:59:33.6351934Z","parent_msg_id":"eb1574eb-85d4-41ab-9828-5b7f35adebb4"},"text/plain":"StatementMeta(, cbff0b2d-78d1-47a2-883f-ee2ebb194048, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.mssparkutilsrunmultiple-result+json":{"activities":[{"duration":48860,"start_time":1759874308826,"session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","created_time":1759874308826,"spark_pool":"Starter Pool","in_pipeline":false,"snapshot_status":"success","notebook_name":"nb_run_workspaces","exception":"Expecting value: line 1 column 1 (char 0)\n---------------------------------------------------------------------------JSONDecodeError                           Traceback (most recent call last)File ~/cluster-env/trident_env/lib/python3.11/site-packages/requests/models.py:971, in Response.json(self, **kwargs)\n    970 try:\n--> 971     return complexjson.loads(self.text, **kwargs)\n    972 except JSONDecodeError as e:\n    973     # Catch JSON-related errors and raise as requests.JSONDecodeError\n    974     # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\nFile ~/cluster-env/trident_env/lib/python3.11/site-packages/simplejson/__init__.py:514, in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\n    510 if (cls is None and encoding is None and object_hook is None and\n    511         parse_int is None and parse_float is None and\n    512         parse_constant is None and object_pairs_hook is None\n    513         and not use_decimal and not allow_nan and not kw):\n--> 514     return _default_decoder.decode(s)\n    515 if cls is None:\nFile ~/cluster-env/trident_env/lib/python3.11/site-packages/simplejson/decoder.py:386, in JSONDecoder.decode(self, s, _w, _PY3)\n    385     s = str(s, self.encoding)\n--> 386 obj, end = self.raw_decode(s)\n    387 end = _w(s, end).end()\nFile ~/cluster-env/trident_env/lib/python3.11/site-packages/simplejson/decoder.py:416, in JSONDecoder.raw_decode(self, s, idx, _w, _PY3)\n    415         idx += 3\n--> 416 return self.scan_once(s, idx=_w(s, idx).end())\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\nDuring handling of the above exception, another exception occurred:\nJSONDecodeError                           Traceback (most recent call last)Cell In[19], line 6\n      4 futures_wu = {executor.submit(process_workspace_users,row): row for row in rows_workspaces}\n      5 for future in as_completed(futures_wu):\n----> 6     result = future.result()\n      7     try:\n      8         # If the function returned a non-null Spark DataFrame, add it to the list\n      9         if result[\"workspace_users\"] is not None:\nFile ~/cluster-env/trident_env/lib/python3.11/concurrent/futures/_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\nFile ~/cluster-env/trident_env/lib/python3.11/concurrent/futures/_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\nFile ~/cluster-env/trident_env/lib/python3.11/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\nCell In[18], line 27, in process_workspace_users(row)\n     24     return {'workspace_users': None}\n     26  # âœ… Add this check to prevent KeyError\n---> 27 if 'value' not in response.json():\n     28     print(f\"âš ï¸ No 'value' key in response for workspace {workspace_name} ({workspace_id})\")\n     29     print(f\"ðŸ” Status Code: {response.status_code} Full response: {response.json()}\")\nFile ~/cluster-env/trident_env/lib/python3.11/site-packages/requests/models.py:975, in Response.json(self, **kwargs)\n    971     return complexjson.loads(self.text, **kwargs)\n    972 except JSONDecodeError as e:\n    973     # Catch JSON-related errors and raise as requests.JSONDecodeError\n    974     # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n--> 975     raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)You can check driver log or snapshot for detailed error info! See how to check logs: https://go.microsoft.com/fwlink/?linkid=2157243 .","end_time":1759874357686,"snapshot_error":"","run_id":"a29fdb67-3011-4c29-beac-0de045b900df","artifact_id":"7582e204-4285-4301-b9d8-89928c44ec76","progress":90,"status":"failure","status_msg":"Failure","activity_name":"nb_run_workspaces","args":{},"exit_value":"","root_artifact_id":"ec4469ba-8228-49ad-a5cf-f78e8277d12c","capacity_id":"2BDAA6BE-16AF-472D-9E6D-7C81AD261EBF","workspace_id":"e54b972a-76a7-4a96-90ab-77441da0157e"},{"duration":47809,"start_time":1759874308839,"session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","created_time":1759874308839,"spark_pool":"Starter Pool","in_pipeline":false,"snapshot_status":"success","notebook_name":"nb_run_tenant_settings","exception":"","end_time":1759874356648,"snapshot_error":"","run_id":"260e9bb4-5788-454b-95d0-f0c969867ca0","artifact_id":"1a58ec05-70e6-4a81-bdb8-5946c66047f5","progress":100,"status":"success","status_msg":"Success","activity_name":"nb_run_tenant_settings","args":{},"exit_value":"","root_artifact_id":"ec4469ba-8228-49ad-a5cf-f78e8277d12c","capacity_id":"2BDAA6BE-16AF-472D-9E6D-7C81AD261EBF","workspace_id":"e54b972a-76a7-4a96-90ab-77441da0157e"},{"duration":47800,"start_time":1759874308835,"session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","created_time":1759874308835,"spark_pool":"Starter Pool","in_pipeline":false,"snapshot_status":"success","notebook_name":"nb_run_capacities","exception":"","end_time":1759874356635,"snapshot_error":"","run_id":"a98d746c-a0f4-46d2-9749-2df6096ca829","artifact_id":"f15c8007-7505-4ea4-acc3-37e9c6775d0b","progress":100,"status":"success","status_msg":"Success","activity_name":"nb_run_capacities","args":{},"exit_value":"","root_artifact_id":"ec4469ba-8228-49ad-a5cf-f78e8277d12c","capacity_id":"2BDAA6BE-16AF-472D-9E6D-7C81AD261EBF","workspace_id":"e54b972a-76a7-4a96-90ab-77441da0157e"},{"duration":48856,"start_time":1759874308822,"session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","created_time":1759874308822,"spark_pool":"Starter Pool","in_pipeline":false,"snapshot_status":"success","notebook_name":"nb_run_dashboards","exception":"","end_time":1759874357678,"snapshot_error":"","run_id":"a4566da8-bbce-4901-9174-0f9fb9c9726b","artifact_id":"68223335-f979-41b5-ad63-13f488b037a5","progress":100,"status":"success","status_msg":"Success","activity_name":"nb_run_dashboards","args":{},"exit_value":"","root_artifact_id":"ec4469ba-8228-49ad-a5cf-f78e8277d12c","capacity_id":"2BDAA6BE-16AF-472D-9E6D-7C81AD261EBF","workspace_id":"e54b972a-76a7-4a96-90ab-77441da0157e"},{"duration":37973,"start_time":1759874308834,"session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","created_time":1759874308834,"spark_pool":"Starter Pool","in_pipeline":false,"snapshot_status":"success","notebook_name":"nb_run_dataflows","exception":"list index out of range\n---------------------------------------------------------------------------IndexError                                Traceback (most recent call last)Cell In[15], line 7\n      2 response = _base_api(\n      3         request=f\"/v1.0/myorg/admin/dataflows\",\n      4         method=\"get\"\n      5     )\n      6 df_dataflows = pd.json_normalize(response.json()['value'])\n----> 7 df_dataflows = spark.createDataFrame(df_dataflows)\n      8 df_dataflows = df_dataflows.drop(\"users\")\nFile /opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1440, in SparkSession.createDataFrame(self, data, schema, samplingRatio, verifySchema)\n   1436     data = pd.DataFrame(data, columns=column_names)\n   1438 if has_pandas and isinstance(data, pd.DataFrame):\n   1439     # Create a DataFrame from pandas DataFrame.\n-> 1440     return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]\n   1441         data, schema, samplingRatio, verifySchema\n   1442     )\n   1443 return self._create_dataframe(\n   1444     data, schema, samplingRatio, verifySchema  # type: ignore[arg-type]\n   1445 )\nFile /opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:362, in SparkConversionMixin.createDataFrame(self, data, schema, samplingRatio, verifySchema)\n    360             warn(msg)\n    361             raise\n--> 362 converted_data = self._convert_from_pandas(data, schema, timezone)\n    363 return self._create_dataframe(converted_data, schema, samplingRatio, verifySchema)\nFile /opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:511, in SparkConversionMixin._convert_from_pandas(self, pdf, schema, timezone)\n    504             pdf[column] = pd.Series(\n    505                 ser.dt.to_pytimedelta(), index=ser.index, dtype=\"object\", name=ser.name\n    506             )\n    508 # Convert pandas.DataFrame to list of numpy records\n    509 np_records = pdf.set_axis(\n    510     [f\"col_{i}\" for i in range(len(pdf.columns))], axis=\"columns\"  # type: ignore[arg-type]\n--> 511 ).to_records(index=False)\n    513 # Check if any columns need to be fixed for Spark to infer properly\n    514 if len(np_records) > 0:\nFile ~/cluster-env/trident_env/lib/python3.11/site-packages/pandas/core/frame.py:2558, in DataFrame.to_records(self, index, column_dtypes, index_dtypes)\n   2555         msg = f\"Invalid dtype {dtype_mapping} specified for {element} {name}\"\n   2556         raise ValueError(msg)\n-> 2558 return np.rec.fromarrays(arrays, dtype={\"names\": names, \"formats\": formats})\nFile ~/cluster-env/trident_env/lib/python3.11/site-packages/numpy/core/records.py:643, in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)\n    640 shape = _deprecate_shape_0_as_None(shape)\n    642 if shape is None:\n--> 643     shape = arrayList[0].shape\n    644 elif isinstance(shape, int):\n    645     shape = (shape,)\nIndexError: list index out of rangeYou can check driver log or snapshot for detailed error info! See how to check logs: https://go.microsoft.com/fwlink/?linkid=2157243 .","end_time":1759874346807,"snapshot_error":"","run_id":"165e8cb1-5f83-435f-ac31-6112448768b8","artifact_id":"11cd15da-2e23-4ea0-a774-8273e4754fff","progress":87,"status":"failure","status_msg":"Failure","activity_name":"nb_run_dataflows","args":{},"exit_value":"","root_artifact_id":"ec4469ba-8228-49ad-a5cf-f78e8277d12c","capacity_id":"2BDAA6BE-16AF-472D-9E6D-7C81AD261EBF","workspace_id":"e54b972a-76a7-4a96-90ab-77441da0157e"},{"duration":47807,"start_time":1759874308824,"session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","created_time":1759874308824,"spark_pool":"Starter Pool","in_pipeline":false,"snapshot_status":"success","notebook_name":"nb_run_reports","exception":"","end_time":1759874356631,"snapshot_error":"","run_id":"1ab6450a-00e4-4d71-9a22-30c2f2b4ead1","artifact_id":"eea9c513-d2ff-49d8-b80c-4c0adaa67c8f","progress":100,"status":"success","status_msg":"Success","activity_name":"nb_run_reports","args":{},"exit_value":"","root_artifact_id":"ec4469ba-8228-49ad-a5cf-f78e8277d12c","capacity_id":"2BDAA6BE-16AF-472D-9E6D-7C81AD261EBF","workspace_id":"e54b972a-76a7-4a96-90ab-77441da0157e"},{"duration":62241,"start_time":1759874308823,"session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","created_time":1759874308823,"spark_pool":"Starter Pool","in_pipeline":false,"snapshot_status":"success","notebook_name":"nb_run_datasets","exception":"","end_time":1759874371064,"snapshot_error":"","run_id":"29a6c7ca-66d8-4214-add8-c8d51ac7cfde","artifact_id":"16457d09-01b4-48d3-a2d3-26b6f7e76927","progress":100,"status":"success","status_msg":"Success","activity_name":"nb_run_datasets","args":{},"exit_value":"","root_artifact_id":"ec4469ba-8228-49ad-a5cf-f78e8277d12c","capacity_id":"2BDAA6BE-16AF-472D-9E6D-7C81AD261EBF","workspace_id":"e54b972a-76a7-4a96-90ab-77441da0157e"},{"duration":35109,"start_time":1759874308825,"session_id":"cbff0b2d-78d1-47a2-883f-ee2ebb194048","created_time":1759874308825,"spark_pool":"Starter Pool","in_pipeline":false,"snapshot_status":"success","notebook_name":"nb_run_activities","exception":"[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset.\n---------------------------------------------------------------------------PySparkValueError                         Traceback (most recent call last)Cell In[16], line 18\n     16 print(f\"Fetching events from {start_time} to {end_time}\")\n     17 df_activities = list_activity_events(start_time, end_time)\n---> 18 df_activities = spark.createDataFrame(df_activities)\n     19 #df_activities.show(1)\n     20 \n     21 # Upsert to Delta\n     22 return_val = udf_UpsertDimension(\n     23     df_activities, \n     24     dimensionType=1,\n   (...)\n     28     flagSoftDeletes=False\n     29 )\nFile /opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1440, in SparkSession.createDataFrame(self, data, schema, samplingRatio, verifySchema)\n   1436     data = pd.DataFrame(data, columns=column_names)\n   1438 if has_pandas and isinstance(data, pd.DataFrame):\n   1439     # Create a DataFrame from pandas DataFrame.\n-> 1440     return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]\n   1441         data, schema, samplingRatio, verifySchema\n   1442     )\n   1443 return self._create_dataframe(\n   1444     data, schema, samplingRatio, verifySchema  # type: ignore[arg-type]\n   1445 )\nFile /opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:363, in SparkConversionMixin.createDataFrame(self, data, schema, samplingRatio, verifySchema)\n    361             raise\n    362 converted_data = self._convert_from_pandas(data, schema, timezone)\n--> 363 return self._create_dataframe(converted_data, schema, samplingRatio, verifySchema)\nFile /opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1485, in SparkSession._create_dataframe(self, data, schema, samplingRatio, verifySchema)\n   1483     rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n   1484 else:\n-> 1485     rdd, struct = self._createFromLocal(map(prepare, data), schema)\n   1486 assert self._jvm is not None\n   1487 jrdd = self._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\nFile /opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1093, in SparkSession._createFromLocal(self, data, schema)\n   1090     data = list(data)\n   1092 if schema is None or isinstance(schema, (list, tuple)):\n-> 1093     struct = self._inferSchemaFromList(data, names=schema)\n   1094     converter = _create_converter(struct)\n   1095     tupled_data: Iterable[Tuple] = map(converter, data)\nFile /opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:948, in SparkSession._inferSchemaFromList(self, data, names)\n    933 \"\"\"\n    934 Infer schema from list of Row, dict, or tuple.\n    935 \n   (...)\n    945 :class:`pyspark.sql.types.StructType`\n    946 \"\"\"\n    947 if not data:\n--> 948     raise PySparkValueError(\n    949         error_class=\"CANNOT_INFER_EMPTY_SCHEMA\",\n    950         message_parameters={},\n    951     )\n    952 infer_dict_as_struct = self._jconf.inferDictAsStruct()\n    953 infer_array_from_first_element = self._jconf.legacyInferArrayTypeFromFirstElement()\nPySparkValueError: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset.You can check driver log or snapshot for detailed error info! See how to check logs: https://go.microsoft.com/fwlink/?linkid=2157243 .","end_time":1759874343934,"snapshot_error":"","run_id":"3aaea743-cdcb-4a59-a785-255fe1324e34","artifact_id":"fe54e0a1-a557-4c4f-adaf-637ff7ecdd55","progress":93,"status":"failure","status_msg":"Failure","activity_name":"nb_run_activities","args":{},"exit_value":"","root_artifact_id":"ec4469ba-8228-49ad-a5cf-f78e8277d12c","capacity_id":"2BDAA6BE-16AF-472D-9E6D-7C81AD261EBF","workspace_id":"e54b972a-76a7-4a96-90ab-77441da0157e"}],"numbers":{"pending":0,"running":0,"failed":3,"succeeded":5},"limit":50}},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"b1987a58-e9e8-40d1-8191-556fd038416c"},{"cell_type":"code","source":["%run nb_udfs"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[12,13,14,15,16,17,18,19,20,21,22],"state":"finished","livy_statement_state":"available","session_id":"fb729e62-ecd8-4ee3-9e6e-8f793c6edade","normalized_state":"finished","queued_time":"2025-05-02T18:03:05.8041539Z","session_start_time":null,"execution_start_time":"2025-05-02T18:03:05.8045609Z","execution_finish_time":"2025-05-02T18:03:10.3890632Z","parent_msg_id":"e93cb32c-ac5b-4905-8a6e-744a2d6d86c8"},"text/plain":"StatementMeta(, fb729e62-ecd8-4ee3-9e6e-8f793c6edade, 22, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"b3c3822d-4a8c-4e1c-8661-f7e3c2e455cb"},{"cell_type":"code","source":["#syncing the sql endpoint to ensure that our reporting can get the latest version of the data\n","\n","udf_SyncSqlEndpoint(workspace,lakehouse)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","session_id":"fb729e62-ecd8-4ee3-9e6e-8f793c6edade","normalized_state":"finished","queued_time":"2025-05-02T14:48:37.8732163Z","session_start_time":null,"execution_start_time":"2025-05-02T18:03:10.3910313Z","execution_finish_time":"2025-05-02T18:03:21.3841042Z","parent_msg_id":"37349ace-b187-43a9-bade-2e0cd0b3e2c9"},"text/plain":"StatementMeta(, fb729e62-ecd8-4ee3-9e6e-8f793c6edade, 23, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Table: dimWorkspaces   Last Update: 2025-05-02T17:45:20.262238Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\nTable: dimDataflows   Last Update: 2025-05-02T06:05:50.9674582Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\nTable: dimReports   Last Update: 2025-05-02T17:45:20.0903587Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\nTable: dimDashboards   Last Update: 2025-04-29T06:04:58.4118239Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\nTable: dimSemanticModels   Last Update: 2025-05-02T17:45:20.418494Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\nTable: dimCapacities   Last Update: 2025-04-26T06:04:46.1961799Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\nTable: dimTenantSettings   Last Update: 2025-04-27T06:04:58.150406Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\nTable: dimWorkspaceUsers   Last Update: 2025-05-02T17:57:09.6487812Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\nTable: factActivities   Last Update: 2025-05-02T06:23:35.7409999Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\nTable: factRefreshHistory   Last Update: 2025-05-02T18:03:08.2085884Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\nTable: factRefreshSchedule   Last Update: 2025-05-02T18:02:54.9632343Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []\n"]},{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"['Table: dimWorkspaces   Last Update: 2025-05-02T17:45:20.262238Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []',\n 'Table: dimDataflows   Last Update: 2025-05-02T06:05:50.9674582Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []',\n 'Table: dimReports   Last Update: 2025-05-02T17:45:20.0903587Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []',\n 'Table: dimDashboards   Last Update: 2025-04-29T06:04:58.4118239Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []',\n 'Table: dimSemanticModels   Last Update: 2025-05-02T17:45:20.418494Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []',\n 'Table: dimCapacities   Last Update: 2025-04-26T06:04:46.1961799Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []',\n 'Table: dimTenantSettings   Last Update: 2025-04-27T06:04:58.150406Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []',\n 'Table: dimWorkspaceUsers   Last Update: 2025-05-02T17:57:09.6487812Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []',\n 'Table: factActivities   Last Update: 2025-05-02T06:23:35.7409999Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []',\n 'Table: factRefreshHistory   Last Update: 2025-05-02T18:03:08.2085884Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []',\n 'Table: factRefreshSchedule   Last Update: 2025-05-02T18:02:54.9632343Z  Table Sync State: NotRun  SQL Sync State: NotRun   Table Warnings: []']"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a714f153-78fd-4f57-a98b-b76fb0fb16fd"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"9b744bc6-b68b-4136-9983-4a665a8d5c9c"}],"default_lakehouse":"9b744bc6-b68b-4136-9983-4a665a8d5c9c","default_lakehouse_name":"lh_monitoring","default_lakehouse_workspace_id":"e54b972a-76a7-4a96-90ab-77441da0157e"}}},"nbformat":4,"nbformat_minor":5}