{"cells":[{"cell_type":"markdown","source":["## Pull in UDFs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"613fd3af-c95c-41e5-90ce-db1ec776e75c"},{"cell_type":"code","source":["%run nb_udfs"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":61,"statement_ids":[51,52,53,54,55,56,57,58,59,60,61],"state":"finished","livy_statement_state":"available","session_id":"b47ebd35-b384-44b7-8e34-d1421a0cfde6","normalized_state":"finished","queued_time":"2025-10-08T02:30:30.447649Z","session_start_time":null,"execution_start_time":"2025-10-08T02:30:30.4480449Z","execution_finish_time":"2025-10-08T02:30:36.0345458Z","parent_msg_id":"0dda65bc-3063-44e9-9fc4-dc9f392234b5"},"text/plain":"StatementMeta(, b47ebd35-b384-44b7-8e34-d1421a0cfde6, 61, Finished, Available, Finished)"},"metadata":{}}],"execution_count":39,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1771e145-6b2f-4b0d-ab76-f3b7552410a3"},{"cell_type":"markdown","source":["Requires the Fabric REST API if you're using Dataflow Gen 2 with CI/CD enabled."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eb684afc-6fe1-4e9a-b74d-08478171f074"},{"cell_type":"markdown","source":["## Run Dataflows"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"76436bdc-cc9a-40f0-9d9b-b873d166d4d6"},{"cell_type":"code","source":["workspace = 'Admin%20Center' #have to escape the & symbol and spaces\n","lakehouse = 'lh_monitoring'\n","\n","dataflow_table = 'dimDataflows'\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":62,"statement_ids":[62],"state":"finished","livy_statement_state":"available","session_id":"b47ebd35-b384-44b7-8e34-d1421a0cfde6","normalized_state":"finished","queued_time":"2025-10-08T02:30:30.2833067Z","session_start_time":null,"execution_start_time":"2025-10-08T02:30:36.0365104Z","execution_finish_time":"2025-10-08T02:30:36.4693744Z","parent_msg_id":"5c753f00-6960-4118-9ec5-26055b7b91fc"},"text/plain":"StatementMeta(, b47ebd35-b384-44b7-8e34-d1421a0cfde6, 62, Finished, Available, Finished)"},"metadata":{}}],"execution_count":40,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"b3335df4-e8ea-402c-8f65-6feda4670c85"},{"cell_type":"code","source":["#get a list of all workspaces and load to a table in the lakehouse\n","response = fab_client.get(f\"/v1/admin/workspaces\")\n","df_workspaces = pd.json_normalize(response.json()['workspaces'])\n","#df_workspaces\n","df_workspaces = spark.createDataFrame(df_workspaces)\n","\n","#creates a list of workspaces we want access to for dataset refresh, history, and workspace users\n","df_np_workspaces = df_workspaces \\\n","    .filter(df_workspaces[\"type\"] == \"Workspace\")  \\\n","    .filter(df_workspaces[\"state\"] ==\"Active\") \\\n","    .withColumnRenamed(\"id\",\"workspaceid\")\n","\n","#df_np_workspaces.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":63,"statement_ids":[63],"state":"finished","livy_statement_state":"available","session_id":"b47ebd35-b384-44b7-8e34-d1421a0cfde6","normalized_state":"finished","queued_time":"2025-10-08T02:30:30.5089495Z","session_start_time":null,"execution_start_time":"2025-10-08T02:30:36.4712706Z","execution_finish_time":"2025-10-08T02:30:37.5514143Z","parent_msg_id":"b38ceafa-b159-43c4-80b2-b1ff5a557489"},"text/plain":"StatementMeta(, b47ebd35-b384-44b7-8e34-d1421a0cfde6, 63, Finished, Available, Finished)"},"metadata":{}}],"execution_count":41,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a87225e9-731a-4397-a9c9-5996ae6f142e"},{"cell_type":"code","source":["\n","# define schema directly\n","schema = StructType([\n","    StructField(\"workspaceId\",  StringType(), True),\n","    StructField(\"dataflowId\",   StringType(), True),\n","    StructField(\"itemType\",     StringType(), True),\n","    StructField(\"displayName\",  StringType(), True),\n","    StructField(\"description\",  StringType(), True),\n","    StructField(\"isParametric\", BooleanType(), True),\n","])\n","\n","rows = []  # we'll append only non-empty responses here\n","\n","for row in (\n","    df_np_workspaces\n","      .select(\"workspaceid\")\n","      .na.drop(subset=[\"workspaceid\"])\n","      .toLocalIterator()\n","):\n","    ws_id = row[\"workspaceid\"]\n","\n","    resp = fab_client.get(f\"/v1/workspaces/{ws_id}/dataflows\")\n","    try:\n","        payload = resp.json()\n","    except Exception as e:\n","        print(f\"{ws_id} -> bad JSON: {e}\")\n","        continue\n","\n","    items = payload.get(\"value\") or []\n","    if not items:\n","        continue  # skip empty responses\n","\n","    for it in items:\n","        rows.append({\n","            \"workspaceId\":  ws_id,\n","            \"dataflowId\":   it.get(\"id\"),\n","            \"itemType\":     it.get(\"type\"),\n","            \"displayName\":  it.get(\"displayName\"),\n","            \"description\":  it.get(\"description\"),\n","            \"isParametric\": (it.get(\"properties\") or {}).get(\"isParametric\"),\n","        })\n","\n","# build a Spark DF\n","df_dataflows = spark.createDataFrame(rows, schema=schema) if rows else spark.createDataFrame([], schema)\n","\n","# sanity check\n","print(f\"Non-empty dataflows found: {df_dataflows.count()}\")\n","df_dataflows.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":64,"statement_ids":[64],"state":"finished","livy_statement_state":"available","session_id":"b47ebd35-b384-44b7-8e34-d1421a0cfde6","normalized_state":"finished","queued_time":"2025-10-08T02:30:30.6149326Z","session_start_time":null,"execution_start_time":"2025-10-08T02:30:37.5534588Z","execution_finish_time":"2025-10-08T02:30:41.1716729Z","parent_msg_id":"482da539-d6b9-407f-adb6-f901b1d6d021"},"text/plain":"StatementMeta(, b47ebd35-b384-44b7-8e34-d1421a0cfde6, 64, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Non-empty dataflows found: 1\n+--------------------+--------------------+--------+---------------+-----------+------------+\n|         workspaceId|          dataflowId|itemType|    displayName|description|isParametric|\n+--------------------+--------------------+--------+---------------+-----------+------------+\n|978cbefd-434d-424...|8f9765b3-75c4-42b...|Dataflow|DF_F-195 Budget|           |       false|\n+--------------------+--------------------+--------+---------------+-----------+------------+\n\n"]}],"execution_count":42,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a9f070c6-b5e6-4410-9638-c9f68e4184a1"},{"cell_type":"code","source":["\n","if df_dataflows.count() !=0:\n","    #creates a slowly changing dimension so we can keep an eye on any deleted dataflows\n","    dataflowPath = udf_GetFilePath(workspace, lakehouse, dataflow_table)\n","    #print(dataflowPath)\n","    naturalKeyColumnList = ['dataflowId']\n","    primaryKeyColumnName = \"tableId\"\n","    returnVal = udf_UpsertDimension(df_dataflows,2,dataflowPath,naturalKeyColumnList,primaryKeyColumnName,True)\n","    print(returnVal)\n","else:\n","    print(\"No dataflows\")\n","    sys.exit(\"No dataflows\")\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":65,"statement_ids":[65],"state":"finished","livy_statement_state":"available","session_id":"b47ebd35-b384-44b7-8e34-d1421a0cfde6","normalized_state":"finished","queued_time":"2025-10-08T02:30:30.7050506Z","session_start_time":null,"execution_start_time":"2025-10-08T02:30:41.1741476Z","execution_finish_time":"2025-10-08T02:30:57.3953667Z","parent_msg_id":"a5d03a1e-515e-45eb-918b-b99f21cd2cd0"},"text/plain":"StatementMeta(, b47ebd35-b384-44b7-8e34-d1421a0cfde6, 65, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ðŸ“¦ Initial load complete\n{'startTime': '2025-10-08 02:30:42.064081', 'stopTime': '2025-10-08 02:30:56.845211', 'details': '0 records updated, 1 records inserted from 1 staging rows to abfss://e54b972a-76a7-4a96-90ab-77441da0157e@onelake.dfs.fabric.microsoft.com/9b744bc6-b68b-4136-9983-4a665a8d5c9c/Tables/dimDataflows'}\n"]}],"execution_count":43,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7d3138ef-f648-4aad-892a-1374bdb982fd"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}