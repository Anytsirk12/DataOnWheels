{"cells":[{"cell_type":"markdown","source":["## Pull in UDFs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f751f83-406a-4d17-ba01-f255bd7d2ddf"},{"cell_type":"code","source":["%run nb_udfs"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":308,"statement_ids":[298,299,300,301,302,303,304,305,306,307,308],"state":"finished","livy_statement_state":"available","session_id":"ab14784b-9cc0-4e5b-a91a-b1b073f0fd01","normalized_state":"finished","queued_time":"2025-10-08T18:23:34.7693371Z","session_start_time":null,"execution_start_time":"2025-10-08T18:23:34.769719Z","execution_finish_time":"2025-10-08T18:23:38.6298885Z","parent_msg_id":"3cf86737-591b-4a25-98a7-1ae4f9b40385"},"text/plain":"StatementMeta(, ab14784b-9cc0-4e5b-a91a-b1b073f0fd01, 308, Finished, Available, Finished)"},"metadata":{}}],"execution_count":86,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"19dec8f6-5b40-43e5-924d-b3059f8d60b9"},{"cell_type":"markdown","source":["## Run Activities"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"af43da5b-e179-4a32-b65f-c6d8220e3c97"},{"cell_type":"code","source":["workspace = 'Admin%20Center' #have to escape the & symbol and spaces\n","lakehouse = 'lh_monitoring'\n","\n","act_table = 'factActivities'\n","date_offset = 1\n","#defaults the activity data to grab yesterday's data. The API only goes back 28 days maximum.\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":309,"statement_ids":[309],"state":"finished","livy_statement_state":"available","session_id":"ab14784b-9cc0-4e5b-a91a-b1b073f0fd01","normalized_state":"finished","queued_time":"2025-10-08T18:23:34.7374055Z","session_start_time":null,"execution_start_time":"2025-10-08T18:23:38.6315402Z","execution_finish_time":"2025-10-08T18:23:38.9227931Z","parent_msg_id":"0afd83fd-f5d2-4542-aa18-8db0bcedbbe8"},"text/plain":"StatementMeta(, ab14784b-9cc0-4e5b-a91a-b1b073f0fd01, 309, Finished, Available, Finished)"},"metadata":{}}],"execution_count":87,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"61bc5a2e-7ce8-4fce-a75c-3b5118130eb2"},{"cell_type":"code","source":["\"\"\"\n","Gets the Power BI activity events for a specific day (offset from today).\n",":param date_offset: Number of days ago (0 = today, 1 = yesterday). Defaults to 1.\n","\"\"\"\n","# Calculate the UTC date string\n","target_date = (dt.utcnow() - timedelta(days=date_offset)).strftime('%Y-%m-%d')\n","start_date = f\"{target_date}T00:00:00.00Z\"\n","end_date = f\"{target_date}T23:59:59.99Z\"\n","print(f\"ðŸ” Fetching activity for {start_date} to {end_date} - date offset {date_offset}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":310,"statement_ids":[310],"state":"finished","livy_statement_state":"available","session_id":"ab14784b-9cc0-4e5b-a91a-b1b073f0fd01","normalized_state":"finished","queued_time":"2025-10-08T18:23:34.8990433Z","session_start_time":null,"execution_start_time":"2025-10-08T18:23:38.9248903Z","execution_finish_time":"2025-10-08T18:23:39.2291905Z","parent_msg_id":"16dcd3c4-ffe6-46bb-9bd9-4357a9419008"},"text/plain":"StatementMeta(, ab14784b-9cc0-4e5b-a91a-b1b073f0fd01, 310, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ðŸ” Fetching activity for 2025-09-10T00:00:00.00Z to 2025-09-10T23:59:59.99Z - date offset 28\n"]}],"execution_count":88,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0997def4-a69d-4e2b-b446-0ed29eefea17"},{"cell_type":"code","source":["# Call the API\n","\n","windows = generate_hourly_windows(target_date)\n","#print(windows)\n","\n","#test = list_activity_events('2025-04-19T00:00:00.000Z','2025-04-19T00:59:59.999Z')\n","#print(test)\n","\n","# Define dimension load config\n","activity_path = udf_GetFilePath(workspace, lakehouse, act_table)\n","natural_key = ['Id']\n","primary_key = 'tableId'\n","\n","\n","for start_time, end_time in windows:\n","    print(f\"Fetching events from {start_time} to {end_time}\")\n","    df_activities = list_activity_events(start_time, end_time)\n","    #print(df_activities)\n","    if df_activities is None or df_activities.empty:\n","        print(\"No activity events in this window; skipping Spark/upsert.\")\n","    else:\n","        df_activities = spark.createDataFrame(df_activities)\n","        #df_activities.show(1)\n","        # Upsert to Delta\n","        return_val = udf_UpsertDimension(\n","            df_activities, \n","            dimensionType=1,\n","            targetPath=activity_path,\n","            naturalKeyColumnList=natural_key,\n","            primaryKeyColumnName=primary_key,\n","            flagSoftDeletes=False\n","        )\n","        print(return_val)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":311,"statement_ids":[311],"state":"finished","livy_statement_state":"cancelled","session_id":"ab14784b-9cc0-4e5b-a91a-b1b073f0fd01","normalized_state":"cancelled","queued_time":"2025-10-08T18:23:35.01523Z","session_start_time":null,"execution_start_time":"2025-10-08T18:23:39.2309916Z","execution_finish_time":"2025-10-08T18:23:48.6776093Z","parent_msg_id":"dceed1f5-06cb-4ccd-adec-575e9b821e56"},"text/plain":"StatementMeta(, ab14784b-9cc0-4e5b-a91a-b1b073f0fd01, 311, Finished, Cancelled, Cancelled)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Fetching events from 2025-09-10T00:00:00.000Z to 2025-09-10T00:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T00:00:00.000Z'&endDateTime='2025-09-10T00:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T00:00:00.000Zâ€“2025-09-10T00:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T01:00:00.000Z to 2025-09-10T01:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T01:00:00.000Z'&endDateTime='2025-09-10T01:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T01:00:00.000Zâ€“2025-09-10T01:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T02:00:00.000Z to 2025-09-10T02:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T02:00:00.000Z'&endDateTime='2025-09-10T02:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T02:00:00.000Zâ€“2025-09-10T02:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T03:00:00.000Z to 2025-09-10T03:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T03:00:00.000Z'&endDateTime='2025-09-10T03:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T03:00:00.000Zâ€“2025-09-10T03:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T04:00:00.000Z to 2025-09-10T04:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T04:00:00.000Z'&endDateTime='2025-09-10T04:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T04:00:00.000Zâ€“2025-09-10T04:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T05:00:00.000Z to 2025-09-10T05:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T05:00:00.000Z'&endDateTime='2025-09-10T05:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T05:00:00.000Zâ€“2025-09-10T05:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T06:00:00.000Z to 2025-09-10T06:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T06:00:00.000Z'&endDateTime='2025-09-10T06:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T06:00:00.000Zâ€“2025-09-10T06:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T07:00:00.000Z to 2025-09-10T07:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T07:00:00.000Z'&endDateTime='2025-09-10T07:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T07:00:00.000Zâ€“2025-09-10T07:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T08:00:00.000Z to 2025-09-10T08:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T08:00:00.000Z'&endDateTime='2025-09-10T08:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T08:00:00.000Zâ€“2025-09-10T08:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T09:00:00.000Z to 2025-09-10T09:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T09:00:00.000Z'&endDateTime='2025-09-10T09:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T09:00:00.000Zâ€“2025-09-10T09:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T10:00:00.000Z to 2025-09-10T10:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T10:00:00.000Z'&endDateTime='2025-09-10T10:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T10:00:00.000Zâ€“2025-09-10T10:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T11:00:00.000Z to 2025-09-10T11:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T11:00:00.000Z'&endDateTime='2025-09-10T11:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T11:00:00.000Zâ€“2025-09-10T11:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T12:00:00.000Z to 2025-09-10T12:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T12:00:00.000Z'&endDateTime='2025-09-10T12:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T12:00:00.000Zâ€“2025-09-10T12:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T13:00:00.000Z to 2025-09-10T13:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T13:00:00.000Z'&endDateTime='2025-09-10T13:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T13:00:00.000Zâ€“2025-09-10T13:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T14:00:00.000Z to 2025-09-10T14:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T14:00:00.000Z'&endDateTime='2025-09-10T14:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T14:00:00.000Zâ€“2025-09-10T14:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T15:00:00.000Z to 2025-09-10T15:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T15:00:00.000Z'&endDateTime='2025-09-10T15:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T15:00:00.000Zâ€“2025-09-10T15:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T16:00:00.000Z to 2025-09-10T16:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T16:00:00.000Z'&endDateTime='2025-09-10T16:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T16:00:00.000Zâ€“2025-09-10T16:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T17:00:00.000Z to 2025-09-10T17:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T17:00:00.000Z'&endDateTime='2025-09-10T17:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T17:00:00.000Zâ€“2025-09-10T17:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T18:00:00.000Z to 2025-09-10T18:59:59.999Z\nâš ï¸ Failed to parse JSON for URL: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-09-10T18:00:00.000Z'&endDateTime='2025-09-10T18:59:59.999Z'\nðŸ” Status code: 400\nðŸ“„ Raw response text: \nâš ï¸ Error in API call for 2025-09-10T18:00:00.000Zâ€“2025-09-10T18:59:59.999Z: Expecting value: line 1 column 1 (char 0)\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T19:00:00.000Z to 2025-09-10T19:59:59.999Z\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T20:00:00.000Z to 2025-09-10T20:59:59.999Z\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T21:00:00.000Z to 2025-09-10T21:59:59.999Z\nNo activity events in this window; skipping Spark/upsert.\nFetching events from 2025-09-10T22:00:00.000Z to 2025-09-10T22:59:59.999Z\n"]},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o140376.collectToPython.\n: org.apache.spark.SparkException: Job 6560 cancelled part of cancelled job group 311\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3086)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2956)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1247)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1246)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3246)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3213)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[932], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m df_activities \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df_activities)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#df_activities.show(1)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Upsert to Delta\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m return_val \u001b[38;5;241m=\u001b[39m udf_UpsertDimension(\n\u001b[1;32m     26\u001b[0m     df_activities, \n\u001b[1;32m     27\u001b[0m     dimensionType\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     28\u001b[0m     targetPath\u001b[38;5;241m=\u001b[39mactivity_path,\n\u001b[1;32m     29\u001b[0m     naturalKeyColumnList\u001b[38;5;241m=\u001b[39mnatural_key,\n\u001b[1;32m     30\u001b[0m     primaryKeyColumnName\u001b[38;5;241m=\u001b[39mprimary_key,\n\u001b[1;32m     31\u001b[0m     flagSoftDeletes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(return_val)\n","Cell \u001b[0;32mIn[920], line 87\u001b[0m, in \u001b[0;36mudf_UpsertDimension\u001b[0;34m(df, dimensionType, targetPath, naturalKeyColumnList, primaryKeyColumnName, flagSoftDeletes)\u001b[0m\n\u001b[1;32m     84\u001b[0m     inserts \u001b[38;5;241m=\u001b[39m inserts\u001b[38;5;241m.\u001b[39munion(updates\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStartDateTime\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_timestamp()))\n\u001b[1;32m     85\u001b[0m     updates \u001b[38;5;241m=\u001b[39m expires\n\u001b[0;32m---> 87\u001b[0m inserts \u001b[38;5;241m=\u001b[39m udf_AddPrimaryKey(inserts, targetPath, primaryKeyColumnName)\n\u001b[1;32m     89\u001b[0m mergeCondition \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AND \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` = source.`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m naturalKeyColumnList])\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Avoid unnecessary count()s\u001b[39;00m\n","Cell \u001b[0;32mIn[911], line 47\u001b[0m, in \u001b[0;36mudf_AddPrimaryKey\u001b[0;34m(df, targetPath, primaryKeyColumnName)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# find the largest ID if the table exists  \u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m notebookutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mexists(targetPath):\n\u001b[0;32m---> 47\u001b[0m     maxID \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload( targetPath )\u001b[38;5;241m.\u001b[39magg({primaryKeyColumnName:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m})\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mselect((maxID \u001b[38;5;241m+\u001b[39m row_number()\u001b[38;5;241m.\u001b[39mover(Window\u001b[38;5;241m.\u001b[39morderBy(lit(\u001b[38;5;28;01mNone\u001b[39;00m))))\u001b[38;5;241m.\u001b[39malias(primaryKeyColumnName)\n\u001b[1;32m     50\u001b[0m         ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m         )\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o140376.collectToPython.\n: org.apache.spark.SparkException: Job 6560 cancelled part of cancelled job group 311\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3086)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2956)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1247)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1246)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3246)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3213)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"]}],"execution_count":89,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c4bf2b38-70fd-4ecc-9297-3190cfeed672"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}